{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/load_data/images\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# data\n",
    "data_dir = pathlib.Path(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_path = str(data_dir.joinpath(\"Corn\", \"metadata.json\"))\n",
    "record_file_path = str(data_dir.joinpath(\"Corn\", \"images.tfrecord\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_file_path) as json_file:\n",
    "    metadata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species': 'Corn',\n",
       " 'num_classes': 4,\n",
       " 'num_files': 2529,\n",
       " 'class_names': ['Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
       "  'Corn_(maize)___Common_rust_',\n",
       "  'Corn_(maize)___Northern_Leaf_Blight',\n",
       "  'Corn_(maize)___healthy'],\n",
       " 'created_date': '2020-01-25 23:37:28'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "image_count = metadata[\"num_files\"]\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "STEPS_PER_EPOCH = np.ceil(image_count / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "def _parse_function(example_proto):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.\n",
    "    image_feature_description = {\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    \n",
    "    # Parse image\n",
    "    parsed_features[\"image\"] = tf.image.decode_jpeg(parsed_features[\"image_raw\"])\n",
    "    #parsed_features[\"image\"] = tf.reshape(image, [image.shape[2], image.shape[0], image.shape[1], 1])\n",
    "    return parsed_features[\"image\"], parsed_features[\"label\"]\n",
    "\n",
    "\n",
    "# Preprocessing: https://docs.databricks.com/applications/deep-learning/data-prep/tfrecords-to-tensorflow.html\n",
    "def normalize(image, label):\n",
    "    \"\"\"Convert `image` from [0, 255] -> [-0.5, 0.5] floats.\"\"\"\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def create_dataset(files, batch_size):\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "    dataset = dataset.map(normalize)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    image, label = iterator.get_next()\n",
    "    image = tf.reshape(image, [batch_size, 300, 300, 3])\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(record_file_path)\n",
    "dataset = dataset.map(_parse_function)\n",
    "# Apply the normalize function\n",
    "dataset = dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module tensorflow.python.keras.engine.sequential:\n",
      "\n",
      "class Sequential(tensorflow.python.keras.engine.training.Model)\n",
      " |  Sequential(layers=None, name=None)\n",
      " |  \n",
      " |  Linear stack of layers.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      layers: list of layers to add to the model.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  # Afterwards, we do automatic shape inference:\n",
      " |  model.add(Dense(32))\n",
      " |  \n",
      " |  # This is identical to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_dim=500))\n",
      " |  \n",
      " |  # And to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, batch_input_shape=(None, 500)))\n",
      " |  \n",
      " |  # Note that you can also omit the `input_shape` argument:\n",
      " |  # In that case the model gets built the first time you call `fit` (or other\n",
      " |  # training and evaluation methods).\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.compile(optimizer=optimizer, loss=loss)\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  \n",
      " |  # Note that when using this delayed-build pattern (no input shape specified),\n",
      " |  # the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns []\n",
      " |  \n",
      " |  # Whereas if you specify the input shape, the model gets built continuously\n",
      " |  # as you are adding layers:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  \n",
      " |  # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  # choose to manually build your model by calling `build(batch_input_shape)`:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.build((None, 500))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      tensorflow.python.keras.engine.training.Model\n",
      " |      tensorflow.python.keras.engine.network.Network\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Builds the model based on input shapes received.\n",
      " |      \n",
      " |      This is to be used for subclassed models, which do not know at instantiation\n",
      " |      time what their inputs look like.\n",
      " |      \n",
      " |      This method only exists for users who want to call `model.build()` in a\n",
      " |      standalone way (as a substitute for calling the model on real data to\n",
      " |      build it). It will never be called by the framework (and thus it will\n",
      " |      never throw unexpected errors in an unrelated workflow).\n",
      " |      \n",
      " |      Args:\n",
      " |       input_shape: Single tuple, TensorShape, or list of shapes, where shapes\n",
      " |           are tuples, integers, or TensorShapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          1. In case of invalid user-provided data (not of type tuple,\n",
      " |             list, or TensorShape).\n",
      " |          2. If the model requires call arguments that are agnostic\n",
      " |             to the input shapes (positional or kwarg in call signature).\n",
      " |          3. If not all layers were properly built.\n",
      " |          4. If float type inputs are not supported within the layers.\n",
      " |      \n",
      " |        In each of these cases, the user should build their model by calling it\n",
      " |        on real tensor data.\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
      " |            the `Network` in training mode or inference mode.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
      " |      Generate class predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A numpy array of class predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
      " |      Generates class probability predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy array of probability predictions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A model instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the network's input specs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, distribute=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See `tf.keras.optimizers`.\n",
      " |          loss: String (name of objective function), objective function or\n",
      " |              `tf.losses.Loss` instance. See `tf.losses`. If the model has\n",
      " |              multiple outputs, you can use a different loss on each output by\n",
      " |              passing a dictionary or a list of losses. The loss value that will\n",
      " |              be minimized by the model will then be the sum of all individual\n",
      " |              losses.\n",
      " |          metrics: List of metrics to be evaluated by the model during training\n",
      " |              and testing. Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary, such as\n",
      " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
      " |              You can also pass a list (len = len(outputs)) of lists of metrics\n",
      " |              such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
      " |              `metrics=['accuracy', ['accuracy', 'mse']]`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          distribute: NOT SUPPORTED IN TF 2.0, please create and compile the\n",
      " |              model under distribution strategy scope instead of passing it to\n",
      " |              compile.\n",
      " |          **kwargs: Any additional arguments.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely).\n",
      " |            If `x` is a dataset, generator or\n",
      " |            `keras.utils.Sequence` instance, `y` should not be specified (since\n",
      " |            targets will be obtained from the iterator/dataset).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` is your data is in the\n",
      " |              form of symbolic tensors, dataset,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      " |              supported when `x` is a dataset, instead pass\n",
      " |              sample weights as the third element of `x`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |              If x is a `tf.data` dataset and `steps` is\n",
      " |              None, 'evaluate' will run until the dataset is exhausted.\n",
      " |              This argument is not supported with array inputs.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during evaluation.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1. If 0, will execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |              or an instance of `keras.utils.Sequence`\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during evaluation.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample weights)`.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
      " |            or `keras.utils.Sequence` instance, `y` should\n",
      " |            not be specified (since targets will be obtained from `x`).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of symbolic tensors, datasets,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              Note that the progress bar is not particularly useful when\n",
      " |              logged to a file, so verbose=2 is recommended when not running\n",
      " |              interactively (eg, in a production environment).\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `tf.keras.callbacks`.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
      " |              not supported when `x` is a dataset, generator or\n",
      " |             `keras.utils.Sequence` instance.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              `validation_data` could be:\n",
      " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      " |                - dataset\n",
      " |              For the first two cases, `batch_size` must be provided.\n",
      " |              For the last case, `validation_steps` must be provided.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      " |              supported when `x` is a dataset, generator, or\n",
      " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      " |              as the third element of `x`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined. If x is a\n",
      " |              `tf.data` dataset, and 'steps_per_epoch'\n",
      " |              is None, the epoch will run until the input dataset is exhausted.\n",
      " |              This argument is not supported with array inputs.\n",
      " |          validation_steps: Only relevant if `validation_data` is provided and\n",
      " |              is a `tf.data` dataset. Total number of steps (batches of\n",
      " |              samples) to draw before stopping when performing validation\n",
      " |              at the end of every epoch. If validation_data is a `tf.data` dataset\n",
      " |              and 'validation_steps' is None, validation\n",
      " |              will run until the `validation_data` dataset is exhausted.\n",
      " |          validation_freq: Only relevant if validation data is provided. Integer\n",
      " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      " |              If an integer, specifies how many training epochs to run before a\n",
      " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
      " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
      " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up\n",
      " |              when using process-based threading. If unspecified, `workers`\n",
      " |              will default to 1. If 0, will execute the generator on the main\n",
      " |              thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |          **kwargs: Used for backwards compatibility.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
      " |      and guarantees the single use of every input per epoch when\n",
      " |      using `use_multiprocessing=True`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: A generator or an instance of `Sequence`\n",
      " |            (`keras.utils.Sequence`)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple `(inputs, targets)`\n",
      " |              - a tuple `(inputs, targets, sample_weights)`.\n",
      " |              This tuple (a single output of the generator) makes a single batch.\n",
      " |              Therefore, all arrays in this tuple must have the same length (equal\n",
      " |              to the size of this batch). Different batches may have different\n",
      " |                sizes.\n",
      " |              For example, the last batch of the epoch is commonly smaller than\n",
      " |                the\n",
      " |              others, if the size of the dataset is not divisible by the batch\n",
      " |                size.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer, total number of iterations on the data.\n",
      " |          verbose: Verbosity mode, 0, 1, or 2.\n",
      " |          callbacks: List of callbacks to be called during training.\n",
      " |          validation_data: This can be either\n",
      " |              - a generator for the validation data\n",
      " |              - a tuple (inputs, targets)\n",
      " |              - a tuple (inputs, targets, sample_weights).\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator. Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          validation_freq: Only relevant if validation data is provided. Integer\n",
      " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      " |              If an integer, specifies how many training epochs to run before a\n",
      " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
      " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
      " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
      " |          class_weight: Dictionary mapping class indices to a weight\n",
      " |              for the class.\n",
      " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (`keras.utils.Sequence`).\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          initial_epoch: Epoch at which to start training\n",
      " |              (useful for resuming a previous training run)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |          def generate_arrays_from_file(path):\n",
      " |              while 1:\n",
      " |                  f = open(path)\n",
      " |                  for line in f:\n",
      " |                      # create numpy arrays of input data\n",
      " |                      # and labels, from each line in the file\n",
      " |                      x1, x2, y = process_line(line)\n",
      " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      " |                  f.close()\n",
      " |      \n",
      " |          model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                              steps_per_epoch=10000, epochs=10)\n",
      " |      ```\n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False)\n",
      " |      Loads all layer weights, either from a TensorFlow or an HDF5 file.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input samples. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` is your data is in the\n",
      " |              form of symbolic tensors, dataset,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
      " |              dataset and `steps` is None, `predict` will\n",
      " |              run until the input dataset is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1. If 0, will execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          generator: Generator yielding batches of input samples\n",
      " |              or an instance of `keras.utils.Sequence` object in order to\n",
      " |              avoid duplicate data when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Maximum size for the generator queue.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of mismatch between given number of inputs and\n",
      " |            expectations of the model.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |      Resets the state of metrics.\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset `y` should\n",
      " |            not be specified (since targets will be obtained from the iterator).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile(). This argument is not\n",
      " |              supported when `x` is a dataset.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |                if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
      " |            `x` is a dataset, `y` should not be specified\n",
      " |            (since targets will be obtained from the iterator).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case of\n",
      " |            temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample. In this case you should make sure to specify\n",
      " |            sample_weight_mode=\"temporal\" in compile(). This argument is not\n",
      " |            supported when `x` is a dataset.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
      " |            weight (float) to apply to the model's loss for the samples from this\n",
      " |            class during training. This can be useful to tell the model to \"pay\n",
      " |            more attention\" to samples from an under-represented class.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  metrics\n",
      " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
      " |  \n",
      " |  metrics_names\n",
      " |      Returns the model's display labels for all outputs.\n",
      " |  \n",
      " |  run_eagerly\n",
      " |      Settable attribute indicating whether the model should run eagerly.\n",
      " |      \n",
      " |      Running eagerly means that your model will be run step by step,\n",
      " |      like Python code. Your model might run slower, but it should become easier\n",
      " |      for you to debug it by stepping into individual layer calls.\n",
      " |      \n",
      " |      By default, we will attempt to compile your model to a static graph to\n",
      " |      deliver the best execution performance.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Boolean, whether the model should run eagerly.\n",
      " |  \n",
      " |  sample_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.network.Network:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None)\n",
      " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to SavedModel or H5 file to save the model.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |          save_format: Either 'tf' or 'h5', indicating whether to save the model\n",
      " |            to Tensorflow SavedModel or HDF5. The default is currently 'h5', but\n",
      " |            will switch to 'tf' in TensorFlow 2.0. The 'tf' option is currently\n",
      " |            disabled (use `tf.keras.experimental.export_saved_model` instead).\n",
      " |        signatures: Signatures to save with the SavedModel. Applicable to the 'tf'\n",
      " |          format only. Please see the `signatures` argument in\n",
      " |          `tf.saved_model.save` for details.\n",
      " |        options: Optional `tf.saved_model.SaveOptions` object that specifies\n",
      " |          options for saving to SavedModel.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True, save_format=None)\n",
      " |      Saves all layer weights.\n",
      " |      \n",
      " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
      " |      argument.\n",
      " |      \n",
      " |      When saving in HDF5 format, the weight file has:\n",
      " |        - `layer_names` (attribute), a list of strings\n",
      " |            (ordered names of model layers).\n",
      " |        - For every layer, a `group` named `layer.name`\n",
      " |            - For every such layer group, a group attribute `weight_names`,\n",
      " |                a list of strings\n",
      " |                (ordered names of weights tensor of the layer).\n",
      " |            - For every weight in the layer, a dataset\n",
      " |                storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      When saving in TensorFlow format, all objects referenced by the network are\n",
      " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
      " |      instances or `Optimizer` instances assigned to object attributes. For\n",
      " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
      " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
      " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
      " |      `Layer` instances must be assigned to object attributes, typically in the\n",
      " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
      " |      `tf.keras.Model` for details.\n",
      " |      \n",
      " |      While the formats are the same, do not mix `save_weights` and\n",
      " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
      " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
      " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
      " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
      " |      `save_weights` for training checkpoints.\n",
      " |      \n",
      " |      The TensorFlow format matches objects and variables by starting at a root\n",
      " |      object, `self` for `save_weights`, and greedily matching attribute\n",
      " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
      " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
      " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
      " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
      " |      the `Model`'s variables. See the [guide to training\n",
      " |      checkpoints](https://www.tensorflow.org/alpha/guide/checkpoints) for details\n",
      " |      on the TensorFlow format.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the file to save the weights to. When saving\n",
      " |              in TensorFlow format, this is the prefix used for checkpoint files\n",
      " |              (multiple files are generated). Note that the '.h5' suffix causes\n",
      " |              weights to be saved in HDF5 format.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
      " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
      " |              `None` defaults to 'tf'.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
      " |              format.\n",
      " |          ValueError: For invalid/unknown format arguments.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use. Defaults to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A YAML string.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: if yaml module is not found.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.network.Network:\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
      " |          `collections`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv2D in module tensorflow.python.keras.layers.convolutional:\n",
      "\n",
      "class Conv2D(Conv)\n",
      " |  Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  2D convolution layer (e.g. spatial convolution over images).\n",
      " |  \n",
      " |  This layer creates a convolution kernel that is convolved\n",
      " |  with the layer input to produce a tensor of\n",
      " |  outputs. If `use_bias` is True,\n",
      " |  a bias vector is created and added to the outputs. Finally, if\n",
      " |  `activation` is not `None`, it is applied to the outputs as well.\n",
      " |  \n",
      " |  When using this layer as the first layer in a model,\n",
      " |  provide the keyword argument `input_shape`\n",
      " |  (tuple of integers, does not include the sample axis),\n",
      " |  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n",
      " |  in `data_format=\"channels_last\"`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    filters: Integer, the dimensionality of the output space\n",
      " |      (i.e. the number of output filters in the convolution).\n",
      " |    kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
      " |      height and width of the 2D convolution window.\n",
      " |      Can be a single integer to specify the same value for\n",
      " |      all spatial dimensions.\n",
      " |    strides: An integer or tuple/list of 2 integers,\n",
      " |      specifying the strides of the convolution along the height and width.\n",
      " |      Can be a single integer to specify the same value for\n",
      " |      all spatial dimensions.\n",
      " |      Specifying any stride value != 1 is incompatible with specifying\n",
      " |      any `dilation_rate` value != 1.\n",
      " |    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |    data_format: A string,\n",
      " |      one of `channels_last` (default) or `channels_first`.\n",
      " |      The ordering of the dimensions in the inputs.\n",
      " |      `channels_last` corresponds to inputs with shape\n",
      " |      `(batch, height, width, channels)` while `channels_first`\n",
      " |      corresponds to inputs with shape\n",
      " |      `(batch, channels, height, width)`.\n",
      " |      It defaults to the `image_data_format` value found in your\n",
      " |      Keras config file at `~/.keras/keras.json`.\n",
      " |      If you never set it, then it will be \"channels_last\".\n",
      " |    dilation_rate: an integer or tuple/list of 2 integers, specifying\n",
      " |      the dilation rate to use for dilated convolution.\n",
      " |      Can be a single integer to specify the same value for\n",
      " |      all spatial dimensions.\n",
      " |      Currently, specifying any `dilation_rate` value != 1 is\n",
      " |      incompatible with specifying any stride value != 1.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to the kernel matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    4D tensor with shape:\n",
      " |    `(samples, channels, rows, cols)` if data_format='channels_first'\n",
      " |    or 4D tensor with shape:\n",
      " |    `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    4D tensor with shape:\n",
      " |    `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
      " |    or 4D tensor with shape:\n",
      " |    `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
      " |    `rows` and `cols` values might have changed due to padding.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv2D\n",
      " |      Conv\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Conv:\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
      " |          `collections`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/processed/Corn/metadata.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(record_file_path).parent.joinpath(\"metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = io.read_metadata(metadata_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = io.read_dataset(record_file_path, 32, metadata[\"num_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 256, 256, 3), (None, 4)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.train_model import model as create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16777344  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 16,871,108\n",
      "Trainable params: 16,871,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(metadata[\"num_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.1437 - accuracy: 0.9156\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 14.2045 - accuracy: 0.7688\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.4810 - accuracy: 0.8938\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0479 - accuracy: 0.9969\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 18s 2s/step - loss: 7.5802 - accuracy: 0.5625\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 2.4084 - accuracy: 0.8344\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 18s 2s/step - loss: 3.2655 - accuracy: 0.6906\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.0565 - accuracy: 0.9937\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 15.1220 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.3223 - accuracy: 0.9656\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1825 - accuracy: 0.9969\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 16s 2s/step - loss: 6.1428 - accuracy: 0.6687\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.1907 - accuracy: 0.7688\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4513 - accuracy: 0.8406\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.0151 - accuracy: 0.9969\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 18s 2s/step - loss: 6.6201e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7150 - accuracy: 0.7531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efb57dbddd0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, steps_per_epoch=10, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/Strawberry___healthy/caa95a07-527a-4ceb-923a-40c717c872b8___RS_HL 1916.JPG',\n",
       " '../data/raw/Strawberry___healthy/275f8963-f4f4-4903-962b-1da716725d08___RS_HL 4780.JPG',\n",
       " '../data/raw/Strawberry___healthy/bb37a7ff-af9b-4db2-ab44-ccd9452e6bf6___RS_HL 2074.JPG',\n",
       " '../data/raw/Strawberry___healthy/3d28c3ea-8419-4e09-addd-211e3828e39f___RS_HL 1942.JPG',\n",
       " '../data/raw/Strawberry___healthy/6b871851-ec5f-4e79-b850-bfdaed9759c5___RS_HL 4446.JPG',\n",
       " '../data/raw/Strawberry___healthy/36fbdab5-6c76-4f04-8252-bfb944c20ddf___RS_HL 4788.JPG',\n",
       " '../data/raw/Strawberry___healthy/7cc70e2f-3727-428a-a414-02dd59f96ae3___RS_HL 4755.JPG',\n",
       " '../data/raw/Strawberry___healthy/3cffaab3-eb47-4ba7-8596-35445be017fc___RS_HL 2217.JPG',\n",
       " '../data/raw/Strawberry___healthy/1788478b-7aac-4ec3-b249-e1d145a39980___RS_HL 2172.JPG',\n",
       " '../data/raw/Strawberry___healthy/bf156be7-0764-4fdf-822a-568a93501825___RS_HL 1886.JPG',\n",
       " '../data/raw/Strawberry___healthy/dc05a60e-5939-4e59-bfee-890db52986cc___RS_HL 2014.JPG',\n",
       " '../data/raw/Strawberry___healthy/fda1f4a2-1416-4f18-a09d-063335c3c58b___RS_HL 1866.JPG',\n",
       " '../data/raw/Strawberry___healthy/b4d26c7f-f0e0-4ab5-bc2e-a47d4285df67___RS_HL 1998.JPG',\n",
       " '../data/raw/Strawberry___healthy/5c7bf62d-3c64-4738-8378-d530b9f73cc1___RS_HL 4517.JPG',\n",
       " '../data/raw/Strawberry___healthy/a950850a-d4ac-4b69-ac73-60633aae3255___RS_HL 4683.JPG',\n",
       " '../data/raw/Strawberry___healthy/90ffc406-a0e7-4f94-98a9-3813e3ce6cc3___RS_HL 1898.JPG',\n",
       " '../data/raw/Strawberry___healthy/fc6a2771-b6f9-4e50-bb37-bbe38792a6cf___RS_HL 1787.JPG',\n",
       " '../data/raw/Strawberry___healthy/d6a987db-813b-4cbf-b0a6-d1ecdbaf06a1___RS_HL 1875.JPG',\n",
       " '../data/raw/Strawberry___healthy/11594f0c-5802-43f4-9281-610a908387d4___RS_HL 2002.JPG',\n",
       " '../data/raw/Strawberry___healthy/aba13626-8beb-474e-b609-300e7eb4e896___RS_HL 1905.JPG',\n",
       " '../data/raw/Strawberry___healthy/0f79c593-bcf2-4a5b-baac-6433f3037a89___RS_HL 2022.JPG',\n",
       " '../data/raw/Strawberry___healthy/24de1e79-2489-4156-8197-72c4b46977bd___RS_HL 4802.JPG',\n",
       " '../data/raw/Strawberry___healthy/f799141e-8ab5-4924-b54a-2b374d22798a___RS_HL 4604.JPG',\n",
       " '../data/raw/Strawberry___healthy/eba54644-386d-4358-88c7-ea6201fe630c___RS_HL 2165.JPG',\n",
       " '../data/raw/Strawberry___healthy/c4eca43a-adf9-4d30-a990-8b177165fb04___RS_HL 4708.JPG',\n",
       " '../data/raw/Strawberry___healthy/00166615-5e7b-4318-8957-5e50df335ee8___RS_HL 1785.JPG',\n",
       " '../data/raw/Strawberry___healthy/83361629-2e1b-425f-ab08-bb9a5e997985___RS_HL 1849.JPG',\n",
       " '../data/raw/Strawberry___healthy/fa64957a-0164-4221-b920-0e68670bf59b___RS_HL 2194.JPG',\n",
       " '../data/raw/Strawberry___healthy/70eb8903-7f36-4787-85db-9c12dc2e02be___RS_HL 2100.JPG',\n",
       " '../data/raw/Strawberry___healthy/426c35b4-2c26-4532-b10a-a21b0e17f355___RS_HL 2063.JPG',\n",
       " '../data/raw/Strawberry___healthy/0d9c97af-4a3e-41f5-9c42-534898ea1688___RS_HL 4375.JPG',\n",
       " '../data/raw/Strawberry___healthy/77c79af8-cfa3-447a-8780-55f9c3836a12___RS_HL 4509.JPG',\n",
       " '../data/raw/Strawberry___healthy/f9371b89-d935-454e-896e-64415815bbe6___RS_HL 1936.JPG',\n",
       " '../data/raw/Strawberry___healthy/6b9a9dd2-74ed-4955-9ef5-68c0a5df09b2___RS_HL 2178.JPG',\n",
       " '../data/raw/Strawberry___healthy/4596027f-5ac4-49f9-bcbd-604d3f5a437a___RS_HL 4790.JPG',\n",
       " '../data/raw/Strawberry___healthy/2b349e9d-0131-444a-acda-9b4154073cb5___RS_HL 4507.JPG',\n",
       " '../data/raw/Strawberry___healthy/422d91ed-7190-40bc-abd9-2aaf6a3082f8___RS_HL 4759.JPG',\n",
       " '../data/raw/Strawberry___healthy/b5bafd06-def2-4fe9-9b04-05a49bfae9b3___RS_HL 4695.JPG',\n",
       " '../data/raw/Strawberry___healthy/d840b66d-1a2d-40fb-8aab-a00836e7b9a2___RS_HL 4660.JPG',\n",
       " '../data/raw/Strawberry___healthy/e4c52c2f-1524-4e9f-b26f-0a15023d487f___RS_HL 1727.JPG',\n",
       " '../data/raw/Strawberry___healthy/4005fb13-0d7c-4a30-9ee3-73e9e4cee05e___RS_HL 1688.JPG',\n",
       " '../data/raw/Strawberry___healthy/7e242504-8dad-42f5-ac47-91f0f4cb3523___RS_HL 2143.JPG',\n",
       " '../data/raw/Strawberry___healthy/0de0d575-5dd7-4fca-b003-d4a13354c89d___RS_HL 4629.JPG',\n",
       " '../data/raw/Strawberry___healthy/2a3f18fd-758e-493f-86c4-cc8edf18b533___RS_HL 1901.JPG',\n",
       " '../data/raw/Strawberry___healthy/5341c472-2e30-4db3-a067-909d7d84bbe4___RS_HL 1818.JPG',\n",
       " '../data/raw/Strawberry___healthy/b286b552-0eb4-437b-8837-ffc5b9e153be___RS_HL 4557.JPG',\n",
       " '../data/raw/Strawberry___healthy/60fba47d-a79f-4653-b6a4-1adeeb2603c0___RS_HL 1871.JPG',\n",
       " '../data/raw/Strawberry___healthy/aca02f14-5945-4687-9f08-7575f061e4b0___RS_HL 4876.JPG',\n",
       " '../data/raw/Strawberry___healthy/37240ad5-3437-49e3-85e0-aec8750e2c39___RS_HL 2064.JPG',\n",
       " '../data/raw/Strawberry___healthy/0cb1928d-bed3-4bb7-8c08-35848eb0f7e8___RS_HL 1718.JPG',\n",
       " '../data/raw/Strawberry___healthy/70c3ebc3-9917-4ed7-920b-e83719e6c414___RS_HL 1926.JPG',\n",
       " '../data/raw/Strawberry___healthy/b37b7fa8-117a-43ae-ac02-d960195833a3___RS_HL 4492.JPG',\n",
       " '../data/raw/Strawberry___healthy/328cd6a7-46a5-4beb-8b62-b68db500a830___RS_HL 1908.JPG',\n",
       " '../data/raw/Strawberry___healthy/abdd34a0-ab02-41e0-95a3-a014ab863ec2___RS_HL 1757.JPG',\n",
       " '../data/raw/Strawberry___healthy/0861080e-6bef-4f10-81d9-7d51cda6b243___RS_HL 1938.JPG',\n",
       " '../data/raw/Strawberry___healthy/cc16d1b1-818c-4a83-a045-5027af514a89___RS_HL 4715.JPG',\n",
       " '../data/raw/Strawberry___healthy/1e007f9a-6e90-4dd6-9be9-f0ce3775128f___RS_HL 2076.JPG',\n",
       " '../data/raw/Strawberry___healthy/1b9a47df-830d-4c52-b7e0-3180c8fee141___RS_HL 4482.JPG',\n",
       " '../data/raw/Strawberry___healthy/de967d7c-a2fb-4b7b-94ef-501b05be474d___RS_HL 4534.JPG',\n",
       " '../data/raw/Strawberry___healthy/d451b14d-8338-47e7-9505-1797ae645742___RS_HL 1976.JPG',\n",
       " '../data/raw/Strawberry___healthy/afc77a38-521f-457d-ba97-1547276d1182___RS_HL 4854.JPG',\n",
       " '../data/raw/Strawberry___healthy/2e164016-5845-40c3-919b-6953bf72a67a___RS_HL 1877.JPG',\n",
       " '../data/raw/Strawberry___healthy/e5c27a76-cadf-4595-927f-c9a097a06587___RS_HL 1950.JPG',\n",
       " '../data/raw/Strawberry___healthy/69d26083-0e91-49a8-990b-1256dce1fdda___RS_HL 1725.JPG',\n",
       " '../data/raw/Strawberry___healthy/77b3d4a4-0712-4e25-bc50-57c12961f7d2___RS_HL 4519.JPG',\n",
       " '../data/raw/Strawberry___healthy/1171891a-bfa5-461f-803c-590f622b0f1c___RS_HL 4382.JPG',\n",
       " '../data/raw/Strawberry___healthy/ab2c9916-41d9-4b71-a678-896575726cf4___RS_HL 1713.JPG',\n",
       " '../data/raw/Strawberry___healthy/e3334bf3-0c75-49c1-a445-90e5525249e3___RS_HL 1759.JPG',\n",
       " '../data/raw/Strawberry___healthy/d75e13ee-2b6a-4457-9a06-acf9fc07dfd9___RS_HL 4385.JPG',\n",
       " '../data/raw/Strawberry___healthy/d13a70dd-9812-4b62-a8f4-3764a50abda3___RS_HL 1796.JPG',\n",
       " '../data/raw/Strawberry___healthy/41dba974-b2b7-4e90-9c93-a8e29200f41c___RS_HL 1659.JPG',\n",
       " '../data/raw/Strawberry___healthy/c0092f7a-c2b9-4895-81eb-9529963ea364___RS_HL 1758.JPG',\n",
       " '../data/raw/Strawberry___healthy/14b39aa2-a081-45a5-ba2d-79d21f9b29a1___RS_HL 1972.JPG',\n",
       " '../data/raw/Strawberry___healthy/243609c1-0a35-4474-8f7c-02dc34044735___RS_HL 1663.JPG',\n",
       " '../data/raw/Strawberry___healthy/ee0f49c6-8884-4e96-8ebc-ceec0b9b69c4___RS_HL 1632.JPG',\n",
       " '../data/raw/Strawberry___healthy/bc26adda-8cdb-43cc-8485-4b2088e233ea___RS_HL 4866.JPG',\n",
       " '../data/raw/Strawberry___healthy/fd929b1c-3f0d-4d0e-96f2-2f99bcb186d9___RS_HL 4728.JPG',\n",
       " '../data/raw/Strawberry___healthy/7cec5131-85c7-498f-87e3-63e4fa847094___RS_HL 1834.JPG',\n",
       " '../data/raw/Strawberry___healthy/fa0ee358-6a10-49f4-900a-6d266ee7b648___RS_HL 2206.JPG',\n",
       " '../data/raw/Strawberry___healthy/4c953897-2924-49cc-a504-e4249b012d18___RS_HL 4869.JPG',\n",
       " '../data/raw/Strawberry___healthy/b938a735-b7a3-463d-b5b9-3bfd7041fe51___RS_HL 1844.JPG',\n",
       " '../data/raw/Strawberry___healthy/ffc5256e-56f2-4448-827a-81bbd1b06be6___RS_HL 4574.JPG',\n",
       " '../data/raw/Strawberry___healthy/057e51f5-f5c1-40b2-930b-a2346c138969___RS_HL 4596.JPG',\n",
       " '../data/raw/Strawberry___healthy/75779559-3218-4930-9687-f2c0e71667c9___RS_HL 2091.JPG',\n",
       " '../data/raw/Strawberry___healthy/f6f867d3-5dee-40fc-a43a-40095a780df8___RS_HL 1858.JPG',\n",
       " '../data/raw/Strawberry___healthy/f076aa8c-cacf-48b0-8a0f-764e7498407d___RS_HL 4826.JPG',\n",
       " '../data/raw/Strawberry___healthy/1f8e3541-9741-49c8-b725-79a00371d404___RS_HL 1675.JPG',\n",
       " '../data/raw/Strawberry___healthy/941d5f1c-8bb9-4027-88b6-3031f1787128___RS_HL 1987.JPG',\n",
       " '../data/raw/Strawberry___healthy/d4405455-8445-4429-94e5-a18d69b10cc1___RS_HL 4396.JPG',\n",
       " '../data/raw/Strawberry___healthy/a654fbca-ecfd-405e-89d0-a7e5c97926ab___RS_HL 4365.JPG',\n",
       " '../data/raw/Strawberry___healthy/bc67ea12-4c8f-443f-a815-87eeeb2e78b3___RS_HL 4476.JPG',\n",
       " '../data/raw/Strawberry___healthy/cf3dedf3-38ff-46bb-b016-5d76a3239914___RS_HL 4414.JPG',\n",
       " '../data/raw/Strawberry___healthy/9435833a-129a-4fe6-830c-e2447faccdc6___RS_HL 4537.JPG',\n",
       " '../data/raw/Strawberry___healthy/5f5c41a6-8da9-4bd9-a4a3-67cdd6c9f300___RS_HL 1912.JPG',\n",
       " '../data/raw/Strawberry___healthy/307ea50d-38f5-4cd5-bfe5-cddc1ad40850___RS_HL 4352.JPG',\n",
       " '../data/raw/Strawberry___healthy/a8e5f630-3a7c-4b71-8b14-0ecdfda521ac___RS_HL 4719.JPG',\n",
       " '../data/raw/Strawberry___healthy/62229adb-1069-4622-8474-7468c668ca16___RS_HL 1637.JPG',\n",
       " '../data/raw/Strawberry___healthy/73df294b-0578-4a9f-a874-05169a2a9b66___RS_HL 4560.JPG',\n",
       " '../data/raw/Strawberry___healthy/8791c247-282a-44b9-a00e-ea9ac08ab3c5___RS_HL 1888.JPG',\n",
       " '../data/raw/Strawberry___healthy/773c7c51-2595-4de4-b4e2-197d1e00f497___RS_HL 4817.JPG',\n",
       " '../data/raw/Strawberry___healthy/db1dd4d2-8424-4b8a-a160-8491b534e6dc___RS_HL 4650.JPG',\n",
       " '../data/raw/Strawberry___healthy/a14512fa-4a18-42cc-9819-97d7f1d87cc3___RS_HL 4798.JPG',\n",
       " '../data/raw/Strawberry___healthy/b798b952-bd98-4484-915d-90f15362d84b___RS_HL 4426.JPG',\n",
       " '../data/raw/Strawberry___healthy/a41cf942-f28c-49a8-8c6f-090bed625de2___RS_HL 2219.JPG',\n",
       " '../data/raw/Strawberry___healthy/f0a6a862-b797-416e-8303-8f45a5d3afc4___RS_HL 4630.JPG',\n",
       " '../data/raw/Strawberry___healthy/ec2361f6-8096-4da7-b238-1050ec3a3890___RS_HL 2235.JPG',\n",
       " '../data/raw/Strawberry___healthy/741e834f-a63a-4efd-b961-d5f7e047abdc___RS_HL 2085.JPG',\n",
       " '../data/raw/Strawberry___healthy/1402ff6e-c678-4c37-b49b-b63374ac0557___RS_HL 1907.JPG',\n",
       " '../data/raw/Strawberry___healthy/76a20a23-24f3-4be6-98b9-6a5567fbeb5e___RS_HL 4868.JPG',\n",
       " '../data/raw/Strawberry___healthy/78debbd4-43b4-437d-8fd8-86910b947d34___RS_HL 4459.JPG',\n",
       " '../data/raw/Strawberry___healthy/d50fa8fa-015b-41f6-a2aa-18efcf041f6e___RS_HL 2188.JPG',\n",
       " '../data/raw/Strawberry___healthy/72b9feaf-3439-483a-bb8a-83395c7a17cd___RS_HL 4782.JPG',\n",
       " '../data/raw/Strawberry___healthy/1d92fcef-79d0-469b-b9b1-71a318505821___RS_HL 4487.JPG',\n",
       " '../data/raw/Strawberry___healthy/0370bc9b-c0c8-49b5-b999-c44323c45216___RS_HL 2202.JPG',\n",
       " '../data/raw/Strawberry___healthy/6980e462-3536-4e42-8924-8eda8c360fdc___RS_HL 4834.JPG',\n",
       " '../data/raw/Strawberry___healthy/a48c8384-32d8-45d2-a734-7224569a40d3___RS_HL 1695.JPG',\n",
       " '../data/raw/Strawberry___healthy/d491c6dc-d208-4199-a764-a98d72a85e5c___RS_HL 1674.JPG',\n",
       " '../data/raw/Strawberry___healthy/8dc1f163-cf56-4951-a9e3-768b383cc5fd___RS_HL 4762.JPG',\n",
       " '../data/raw/Strawberry___healthy/873c27c3-fcf9-4ad1-bcfa-9865b2c76922___RS_HL 4422.JPG',\n",
       " '../data/raw/Strawberry___healthy/23d4fa2f-a966-44a3-a597-b39a79618378___RS_HL 1744.JPG',\n",
       " '../data/raw/Strawberry___healthy/343c0bf7-2f84-47e0-8229-e46cadee8040___RS_HL 2200.JPG',\n",
       " '../data/raw/Strawberry___healthy/941b6567-8f4e-4092-a954-911fbde578d7___RS_HL 2040.JPG',\n",
       " '../data/raw/Strawberry___healthy/968be429-8b72-401a-ba28-9552faf35e66___RS_HL 1921.JPG',\n",
       " '../data/raw/Strawberry___healthy/a0e41138-6377-41d2-a339-94ad75279f95___RS_HL 1896.JPG',\n",
       " '../data/raw/Strawberry___healthy/ee80b097-e9de-45ef-8c65-69b03a67c809___RS_HL 2152.JPG',\n",
       " '../data/raw/Strawberry___healthy/50eb9d31-53f5-4040-b83f-681da8a8f621___RS_HL 1815.JPG',\n",
       " '../data/raw/Strawberry___healthy/78d75cc6-54e0-469d-ade8-d67d1752872a___RS_HL 2018.JPG',\n",
       " '../data/raw/Strawberry___healthy/de6a28bb-b09b-4279-b31b-f716fa637356___RS_HL 1997.JPG',\n",
       " '../data/raw/Strawberry___healthy/35207f57-999d-4698-94aa-2d8c4efb230f___RS_HL 4726.JPG',\n",
       " '../data/raw/Strawberry___healthy/20dab82c-afc9-4e70-9334-b9245de1f537___RS_HL 2139.JPG',\n",
       " '../data/raw/Strawberry___healthy/ec4ad3dd-4feb-4027-8cab-3cca8428eeb4___RS_HL 4705.JPG',\n",
       " '../data/raw/Strawberry___healthy/fe9084a4-e32b-4d45-8249-db682f4c6ca8___RS_HL 1779.JPG',\n",
       " '../data/raw/Strawberry___healthy/2ac2f01a-2009-4d77-ae77-e3925f513fe0___RS_HL 4443.JPG',\n",
       " '../data/raw/Strawberry___healthy/caa6652b-d711-46cb-a216-70028258982f___RS_HL 1948.JPG',\n",
       " '../data/raw/Strawberry___healthy/67db4fa4-922e-47f8-bf79-7dbb87adf47b___RS_HL 1986.JPG',\n",
       " '../data/raw/Strawberry___healthy/961cb53b-3ad0-4b8c-9de3-3bfec73ea444___RS_HL 4340.JPG',\n",
       " '../data/raw/Strawberry___healthy/9a894681-b411-4940-b60e-54ea17cde765___RS_HL 4496.JPG',\n",
       " '../data/raw/Strawberry___healthy/0400adad-86e7-4e42-9141-acc0f226e2e7___RS_HL 2111.JPG',\n",
       " '../data/raw/Strawberry___healthy/da4dcda4-9c5e-43e5-a19d-f13b53be68fc___RS_HL 2130.JPG',\n",
       " '../data/raw/Strawberry___healthy/ef5cbfe5-8ba7-4291-8246-27219808808a___RS_HL 1842.JPG',\n",
       " '../data/raw/Strawberry___healthy/400f7591-91a9-4835-ab0d-d5c0fc210a54___RS_HL 1776.JPG',\n",
       " '../data/raw/Strawberry___healthy/a67277f1-75b5-41c5-8de1-dad85fbc42a4___RS_HL 2086.JPG',\n",
       " '../data/raw/Strawberry___healthy/20a44f54-5534-4eed-9240-b1a6eb8ce4e1___RS_HL 1708.JPG',\n",
       " '../data/raw/Strawberry___healthy/b3698114-5399-4cfe-b585-252b2a63ff64___RS_HL 4654.JPG',\n",
       " '../data/raw/Strawberry___healthy/6f058362-f5b8-4602-b1ee-0e13143658f7___RS_HL 4513.JPG',\n",
       " '../data/raw/Strawberry___healthy/4e4798cc-ae7d-4480-8c7d-2818b2b6a8fa___RS_HL 4417.JPG',\n",
       " '../data/raw/Strawberry___healthy/2fbe071f-efd0-4a93-97b5-38160dafa0c5___RS_HL 2146.JPG',\n",
       " '../data/raw/Strawberry___healthy/4d24a8c7-2a7a-4598-a673-e6c1e856dc23___RS_HL 4469.JPG',\n",
       " '../data/raw/Strawberry___healthy/768de271-ba00-4cfe-b785-ccc7deedbcce___RS_HL 2081.JPG',\n",
       " '../data/raw/Strawberry___healthy/2a7a5c9d-1066-4af5-8560-30f53f9d5b33___RS_HL 4821.JPG',\n",
       " '../data/raw/Strawberry___healthy/a53942e6-2216-49a0-b09b-5d8a72bf7a39___RS_HL 4627.JPG',\n",
       " '../data/raw/Strawberry___healthy/feea7aff-b70c-46dd-8ff4-be06e7920fa9___RS_HL 1807.JPG',\n",
       " '../data/raw/Strawberry___healthy/5f13420e-16bb-4aa8-9869-051e0b937ec1___RS_HL 4543.JPG',\n",
       " '../data/raw/Strawberry___healthy/56bce402-3d38-4f90-b20c-72886a47a072___RS_HL 1704.JPG',\n",
       " '../data/raw/Strawberry___healthy/8c7b0f08-a935-47d6-8060-c4223c7c1be1___RS_HL 4801.JPG',\n",
       " '../data/raw/Strawberry___healthy/4673c301-00c4-4953-95e0-5badca7ff0a3___RS_HL 4661.JPG',\n",
       " '../data/raw/Strawberry___healthy/1b250776-ccb2-49b9-9964-a729567f60b9___RS_HL 2205.JPG',\n",
       " '../data/raw/Strawberry___healthy/57487bb3-ceeb-4455-a35b-2a9c53b26feb___RS_HL 1774.JPG',\n",
       " '../data/raw/Strawberry___healthy/ed89a94c-9730-4607-8ad4-c23ffb0a8ffb___RS_HL 4532.JPG',\n",
       " '../data/raw/Strawberry___healthy/8df11a1a-0e18-4757-848b-e3901e9fbd2f___RS_HL 1626.JPG',\n",
       " '../data/raw/Strawberry___healthy/27380fd9-9d5d-41b2-b4a4-f1c1f1fd58ab___RS_HL 1712.JPG',\n",
       " '../data/raw/Strawberry___healthy/1a28038d-0e6d-4ac7-94d1-76d30fc89d71___RS_HL 2058.JPG',\n",
       " '../data/raw/Strawberry___healthy/7ba8b9d1-9f10-479c-901a-b038f042f0ce___RS_HL 4749.JPG',\n",
       " '../data/raw/Strawberry___healthy/7e0587c2-c806-4e7c-a3d6-53b810de904d___RS_HL 4748.JPG',\n",
       " '../data/raw/Strawberry___healthy/4c3cc96f-e1d9-4237-b72d-2a1ea0a755f8___RS_HL 2167.JPG',\n",
       " '../data/raw/Strawberry___healthy/e0ca989a-cad6-422c-81e3-803b48ad9945___RS_HL 1635.JPG',\n",
       " '../data/raw/Strawberry___healthy/6c96f05e-d43d-4715-b634-df8dbc32c6d7___RS_HL 2125.JPG',\n",
       " '../data/raw/Strawberry___healthy/ebd8d3f6-aa1e-4cc0-acb0-3e6c1ceb4bae___RS_HL 4379.JPG',\n",
       " '../data/raw/Strawberry___healthy/2e75a60a-af82-45cf-abe0-1d13d63dc6fa___RS_HL 2017.JPG',\n",
       " '../data/raw/Strawberry___healthy/15b5d300-a3f7-4e46-9f33-47f99c5d8364___RS_HL 4832.JPG',\n",
       " '../data/raw/Strawberry___healthy/a2a27584-137c-4cd7-bb60-90a3f90c461e___RS_HL 1795.JPG',\n",
       " '../data/raw/Strawberry___healthy/a5db3b2a-9806-45c1-b1f8-9d6f414222f2___RS_HL 2067.JPG',\n",
       " '../data/raw/Strawberry___healthy/98cd6b17-6eed-454c-9fa4-1fdd22370d99___RS_HL 4589.JPG',\n",
       " '../data/raw/Strawberry___healthy/675046ac-bf24-4ac4-ae18-0623ed00ec3f___RS_HL 4829.JPG',\n",
       " '../data/raw/Strawberry___healthy/6829e297-83c4-4032-9fb2-34baf6bb64e7___RS_HL 4576.JPG',\n",
       " '../data/raw/Strawberry___healthy/04cb725a-632d-44e0-9886-c84e2bbdd3da___RS_HL 2094.JPG',\n",
       " '../data/raw/Strawberry___healthy/c3d65977-d35a-4f0f-9e36-bf36f4dbdee8___RS_HL 2214.JPG',\n",
       " '../data/raw/Strawberry___healthy/35166a03-4a2c-494b-8ca9-b5224edde194___RS_HL 1991.JPG',\n",
       " '../data/raw/Strawberry___healthy/dd6cb157-def3-4bcc-917c-4815199655a2___RS_HL 1915.JPG',\n",
       " '../data/raw/Strawberry___healthy/677ba255-d1ac-48b7-9847-dd9abc1a8db8___RS_HL 1887.JPG',\n",
       " '../data/raw/Strawberry___healthy/323bde1b-bd54-4af8-b99d-dd3cf6be24bc___RS_HL 2110.JPG',\n",
       " '../data/raw/Strawberry___healthy/2ec57765-dacf-4ed3-b753-eff3fe27a3d1___RS_HL 4763.JPG',\n",
       " '../data/raw/Strawberry___healthy/091b0459-1a9a-4a60-b59f-3de2faedb6f8___RS_HL 2101.JPG',\n",
       " '../data/raw/Strawberry___healthy/bc365494-4c30-4a7e-8ecc-0e9876ce313f___RS_HL 4716.JPG',\n",
       " '../data/raw/Strawberry___healthy/47f2b614-506f-469d-a939-a081d056d3b5___RS_HL 1863.JPG',\n",
       " '../data/raw/Strawberry___healthy/f23cfb59-4e9d-43b0-820a-99d9cc624c04___RS_HL 4618.JPG',\n",
       " '../data/raw/Strawberry___healthy/08dd3504-4ea2-4afb-89ac-c666f3016b94___RS_HL 4619.JPG',\n",
       " '../data/raw/Strawberry___healthy/c4605ff1-113a-4d78-b318-e301d80c0b7d___RS_HL 1644.JPG',\n",
       " '../data/raw/Strawberry___healthy/878d53af-ce12-4389-948e-6a75a3654f4a___RS_HL 4578.JPG',\n",
       " '../data/raw/Strawberry___healthy/15a5c256-859d-4cff-8b3f-597b8baf3329___RS_HL 4843.JPG',\n",
       " '../data/raw/Strawberry___healthy/52f02bf8-07cd-4c64-af80-62d4cf0fad92___RS_HL 4662.JPG',\n",
       " '../data/raw/Strawberry___healthy/998f4c67-2e7e-4881-a865-4ce2dc940a43___RS_HL 1851.JPG',\n",
       " '../data/raw/Strawberry___healthy/92b561cb-8839-4da0-81af-faedcb91b788___RS_HL 2193.JPG',\n",
       " '../data/raw/Strawberry___healthy/a17e91e2-3daf-4f30-8976-9ea23b6bdf94___RS_HL 2053.JPG',\n",
       " '../data/raw/Strawberry___healthy/7d6ed1f3-f7c9-48a8-a363-bf2ea58f3f67___RS_HL 4839.JPG',\n",
       " '../data/raw/Strawberry___healthy/873b6e5a-62b6-4725-bd1d-8d33888f8f2e___RS_HL 4860.JPG',\n",
       " '../data/raw/Strawberry___healthy/b6c7ab0a-7c78-4c75-aa71-878972b5a4b4___RS_HL 4569.JPG',\n",
       " '../data/raw/Strawberry___healthy/930e633e-e66c-4a7c-ad3d-1a3c7666c176___RS_HL 2210.JPG',\n",
       " '../data/raw/Strawberry___healthy/cd58ce68-c194-4171-aedb-2525d0c0d813___RS_HL 4612.JPG',\n",
       " '../data/raw/Strawberry___healthy/5d541b79-2861-493a-b121-c4f0a60be3d2___RS_HL 4607.JPG',\n",
       " '../data/raw/Strawberry___healthy/80372fb3-2a02-4989-9396-6732cd47cb75___RS_HL 1623.JPG',\n",
       " '../data/raw/Strawberry___healthy/64aea8c6-24df-40c1-9d68-0221f4151383___RS_HL 2103.JPG',\n",
       " '../data/raw/Strawberry___healthy/963dd16b-956b-44c4-b2e8-bfdd1beed6a9___RS_HL 4371.JPG',\n",
       " '../data/raw/Strawberry___healthy/ec906a90-7da2-420f-a3ab-5a67979868ee___RS_HL 2075.JPG',\n",
       " '../data/raw/Strawberry___healthy/65880698-1582-43e9-86c4-3dfcea34ee8d___RS_HL 1654.JPG',\n",
       " '../data/raw/Strawberry___healthy/550c54e7-c80e-4225-8060-b20af93458ff___RS_HL 1640.JPG',\n",
       " '../data/raw/Strawberry___healthy/08ac4a71-35c5-45c5-8e49-20427b6e53db___RS_HL 4362.JPG',\n",
       " '../data/raw/Strawberry___healthy/a164ad90-be59-4ed8-948e-4b734e926490___RS_HL 4645.JPG',\n",
       " '../data/raw/Strawberry___healthy/bf2896d7-8289-4198-ae01-c8870ccda768___RS_HL 2204.JPG',\n",
       " '../data/raw/Strawberry___healthy/65ce255d-cc5b-43e3-b44a-5b65b8230e07___RS_HL 1784.JPG',\n",
       " '../data/raw/Strawberry___healthy/57c57efe-e32c-447b-b60d-cdedf0f86b28___RS_HL 4673.JPG',\n",
       " '../data/raw/Strawberry___healthy/4670637d-5678-48d5-9347-079e65734650___RS_HL 4750.JPG',\n",
       " '../data/raw/Strawberry___healthy/98e1c44c-f11b-4e0c-9450-c465feb62846___RS_HL 4770.JPG',\n",
       " '../data/raw/Strawberry___healthy/4d5733cd-e68e-4fd8-9587-aaaf9502934b___RS_HL 4466.JPG',\n",
       " '../data/raw/Strawberry___healthy/fd603558-801c-43bc-94c7-07bcd78814a5___RS_HL 4485.JPG',\n",
       " '../data/raw/Strawberry___healthy/dc2e47b9-4c1e-42de-be39-3ebca2fa356d___RS_HL 1699.JPG',\n",
       " '../data/raw/Strawberry___healthy/ce36f5b8-cffb-49d5-9cf1-e8505de98fc6___RS_HL 1649.JPG',\n",
       " '../data/raw/Strawberry___healthy/6c4bdcf6-fb2d-4c79-b389-ca44be997a98___RS_HL 2220.JPG',\n",
       " '../data/raw/Strawberry___healthy/765d2e6e-9011-41c3-b2aa-9ed31601f223___RS_HL 1859.JPG',\n",
       " '../data/raw/Strawberry___healthy/9dcb731c-e7fa-4193-a844-226a17ca04e9___RS_HL 4479.JPG',\n",
       " '../data/raw/Strawberry___healthy/a32f24fa-91ce-42dd-b786-5e4ed4f2d037___RS_HL 4501.JPG',\n",
       " '../data/raw/Strawberry___healthy/15bd3182-f1d2-4d75-b54e-9aca616cec66___RS_HL 2077.JPG',\n",
       " '../data/raw/Strawberry___healthy/17a7354e-4d31-4b34-8efa-adc1a370b201___RS_HL 4692.JPG',\n",
       " '../data/raw/Strawberry___healthy/fdc2ad6e-3ed2-46be-b862-2475be5cc19a___RS_HL 4570.JPG',\n",
       " '../data/raw/Strawberry___healthy/73cc96dc-59d4-4a43-91ed-8286de00beea___RS_HL 4593.JPG',\n",
       " '../data/raw/Strawberry___healthy/cc31bcec-0229-4fe2-9a1f-7dbb10d70a1d___RS_HL 1620.JPG',\n",
       " '../data/raw/Strawberry___healthy/a2a51632-4ca8-4579-8a75-f2b4188e1f59___RS_HL 2181.JPG',\n",
       " '../data/raw/Strawberry___healthy/ac88dc9c-7333-4871-8e85-25279d2a552e___RS_HL 4388.JPG',\n",
       " '../data/raw/Strawberry___healthy/76966414-7ff1-4a03-b8a8-e4b7bd7ed106___RS_HL 2026.JPG',\n",
       " '../data/raw/Strawberry___healthy/1ec65a98-9f03-4523-90c1-a0d1e238b5a8___RS_HL 4428.JPG',\n",
       " '../data/raw/Strawberry___healthy/9c969fa3-09f8-4d4e-bd3d-77f009602d39___RS_HL 2037.JPG',\n",
       " '../data/raw/Strawberry___healthy/4abe583a-841b-4173-98d5-548dcbc16275___RS_HL 2129.JPG',\n",
       " '../data/raw/Strawberry___healthy/d93424d7-a8db-4cff-b534-2f47930950fa___RS_HL 1745.JPG',\n",
       " '../data/raw/Strawberry___healthy/fea9a141-9561-4508-bd7d-49cd9412bd2f___RS_HL 1624.JPG',\n",
       " '../data/raw/Strawberry___healthy/4197c213-9bf6-4b26-aff1-77a73c45ae1a___RS_HL 2117.JPG',\n",
       " '../data/raw/Strawberry___healthy/d564ec62-6533-4d66-98d6-f60f097d8f6c___RS_HL 4824.JPG',\n",
       " '../data/raw/Strawberry___healthy/b7006ce4-c162-4b5c-bd74-78ea0a63a558___RS_HL 1939.JPG',\n",
       " '../data/raw/Strawberry___healthy/f34a22a5-fa67-436d-872e-bdff52296a9b___RS_HL 2148.JPG',\n",
       " '../data/raw/Strawberry___healthy/1a119565-0295-4692-95cd-fbc32728f855___RS_HL 1853.JPG',\n",
       " '../data/raw/Strawberry___healthy/e53150b0-d1a7-4af6-8058-f3f0326d80f4___RS_HL 1633.JPG',\n",
       " '../data/raw/Strawberry___healthy/fd530597-32df-4d80-bae0-4af7a8ebcb92___RS_HL 4771.JPG',\n",
       " '../data/raw/Strawberry___healthy/b127a511-d0ce-4fdc-bbe6-6c2a2e2dcf1f___RS_HL 1816.JPG',\n",
       " '../data/raw/Strawberry___healthy/812cfed0-4517-4025-9163-cddc549b93cf___RS_HL 4756.JPG',\n",
       " '../data/raw/Strawberry___healthy/666a3c73-655c-43e4-a4d3-a9cff6a31b84___RS_HL 4331.JPG',\n",
       " '../data/raw/Strawberry___healthy/6419aea0-0a20-4ec6-bda4-5fcdda4eadd8___RS_HL 1930.JPG',\n",
       " '../data/raw/Strawberry___healthy/1345a880-f629-49ea-8524-234770fe3cc6___RS_HL 4529.JPG',\n",
       " '../data/raw/Strawberry___healthy/9e5aab8b-acca-4647-9dba-e4c8c7f13ad7___RS_HL 4682.JPG',\n",
       " '../data/raw/Strawberry___healthy/65b83aea-55da-4f73-a4bd-ee81843e436b___RS_HL 1651.JPG',\n",
       " '../data/raw/Strawberry___healthy/c15bc656-8360-40c4-8e2b-3b5524a93e3a___RS_HL 1981.JPG',\n",
       " '../data/raw/Strawberry___healthy/8e5be35a-21e9-4031-83a7-26b09f0cae7a___RS_HL 4472.JPG',\n",
       " '../data/raw/Strawberry___healthy/7492e688-82ea-4de0-98ca-8cd648d64d20___RS_HL 2162.JPG',\n",
       " '../data/raw/Strawberry___healthy/64939151-e6df-4236-8ba4-51ac9bcd7795___RS_HL 1811.JPG',\n",
       " '../data/raw/Strawberry___healthy/23471535-c155-435a-b538-a1647bb13e08___RS_HL 4742.JPG',\n",
       " '../data/raw/Strawberry___healthy/07a3dfc9-da98-42e3-a237-f5fa518d1f3b___RS_HL 1711.JPG',\n",
       " '../data/raw/Strawberry___healthy/974f90d8-f19f-4174-8cfc-a1f658e322a5___RS_HL 4669.JPG',\n",
       " '../data/raw/Strawberry___healthy/9bedf987-74de-4682-9458-61b01d98a26f___RS_HL 1876.JPG',\n",
       " '../data/raw/Strawberry___healthy/29050f21-a393-473e-9f9c-7fd99feef9a7___RS_HL 4533.JPG',\n",
       " '../data/raw/Strawberry___healthy/525ea4dd-84e2-4a43-9927-41cd69543db4___RS_HL 4457.JPG',\n",
       " '../data/raw/Strawberry___healthy/0db3e0a5-45c6-4cb4-96ec-943862dad781___RS_HL 1817.JPG',\n",
       " '../data/raw/Strawberry___healthy/26d742d2-76d6-41bb-ba8e-ea068793d74c___RS_HL 4334.JPG',\n",
       " '../data/raw/Strawberry___healthy/8b799a37-bc78-416f-9081-1ac2d8caba6b___RS_HL 2190.JPG',\n",
       " '../data/raw/Strawberry___healthy/b30220c5-0c39-4953-8e0b-1a2236fd4a8b___RS_HL 4399.JPG',\n",
       " '../data/raw/Strawberry___healthy/2f8b9436-12dd-40d2-9301-3a14eff854dc___RS_HL 1733.JPG',\n",
       " '../data/raw/Strawberry___healthy/fad0fc0b-4619-422e-8c59-d5bab6469af4___RS_HL 4737.JPG',\n",
       " '../data/raw/Strawberry___healthy/904b163c-bb30-4fe5-bf79-d19ddb7adbba___RS_HL 1951.JPG',\n",
       " '../data/raw/Strawberry___healthy/d41bf2be-46ed-4d79-bb75-8f690ba1b332___RS_HL 4545.JPG',\n",
       " '../data/raw/Strawberry___healthy/f5c2a553-b9af-4141-a6ba-5a4af090db7d___RS_HL 2068.JPG',\n",
       " '../data/raw/Strawberry___healthy/95353a0f-fa08-4055-8fa2-a82c8470ced8___RS_HL 4511.JPG',\n",
       " '../data/raw/Strawberry___healthy/d94430d8-1f02-41c5-97b0-2fefd03ae088___RS_HL 4871.JPG',\n",
       " '../data/raw/Strawberry___healthy/15b3921b-acae-4865-b99e-f4fe81c84e7d___RS_HL 1724.JPG',\n",
       " '../data/raw/Strawberry___healthy/09b43094-79ca-4404-aafc-4c93eac76e06___RS_HL 4347.JPG',\n",
       " '../data/raw/Strawberry___healthy/da93ba82-4070-4373-b818-a6fea8c91351___RS_HL 4724.JPG',\n",
       " '../data/raw/Strawberry___healthy/64531481-3721-4073-879e-d1db62204a2f___RS_HL 1932.JPG',\n",
       " '../data/raw/Strawberry___healthy/c1ccdd49-6389-40eb-b841-c77ab6568014___RS_HL 2056.JPG',\n",
       " '../data/raw/Strawberry___healthy/730b27b1-51a1-4c14-81f8-ab1da6237c4f___RS_HL 2051.JPG',\n",
       " '../data/raw/Strawberry___healthy/e178986b-223c-4da1-8c0f-2314de66df55___RS_HL 2042.JPG',\n",
       " '../data/raw/Strawberry___healthy/4a11d51c-32a7-41b1-b089-8caf4c467b02___RS_HL 4815.JPG',\n",
       " '../data/raw/Strawberry___healthy/b973706d-80b9-43bc-bcc7-c451bf0e7835___RS_HL 2243.JPG',\n",
       " '../data/raw/Strawberry___healthy/71d3e6d6-4810-4fbe-9b58-ff339b0a62ab___RS_HL 1751.JPG',\n",
       " '../data/raw/Strawberry___healthy/f1f33667-e880-4980-92ba-2401f2954ba8___RS_HL 1690.JPG',\n",
       " '../data/raw/Strawberry___healthy/d07e9834-ac76-4390-9463-cbef9d0269da___RS_HL 1736.JPG',\n",
       " '../data/raw/Strawberry___healthy/562ea8c6-b573-4f3a-913f-7b621c77d312___RS_HL 2198.JPG',\n",
       " '../data/raw/Strawberry___healthy/30b0611b-c150-4668-8007-662129beaa6e___RS_HL 1806.JPG',\n",
       " '../data/raw/Strawberry___healthy/de5c0c35-f697-421f-8f4f-937185e6a803___RS_HL 4786.JPG',\n",
       " '../data/raw/Strawberry___healthy/30c218c0-6fb6-45d8-a645-43be89932d06___RS_HL 2005.JPG',\n",
       " '../data/raw/Strawberry___healthy/02ecedef-e743-4909-a000-bff6ff373b6c___RS_HL 2222.JPG',\n",
       " '../data/raw/Strawberry___healthy/ab5ba6b4-ed4e-4dae-a497-1bc4f3688313___RS_HL 2177.JPG',\n",
       " '../data/raw/Strawberry___healthy/33d585b6-736d-4767-9004-2444f4916fd9___RS_HL 4360.JPG',\n",
       " '../data/raw/Strawberry___healthy/6b8e87b2-5438-4acd-a4f9-0a1442a766f5___RS_HL 4806.JPG',\n",
       " '../data/raw/Strawberry___healthy/75d24cc5-b37f-42de-947f-dc7ee673674e___RS_HL 4571.JPG',\n",
       " '../data/raw/Strawberry___healthy/3721986d-7a03-40e5-8602-7483704c5560___RS_HL 2169.JPG',\n",
       " '../data/raw/Strawberry___healthy/06d4ec77-12ef-47ed-ab37-b181fb1b03bf___RS_HL 4647.JPG',\n",
       " '../data/raw/Strawberry___healthy/4beddcd4-6c36-4e8e-9ec6-34bcd7d9d11e___RS_HL 4623.JPG',\n",
       " '../data/raw/Strawberry___healthy/b549b207-9f09-46cb-a2f6-bc4ffca961bf___RS_HL 1831.JPG',\n",
       " '../data/raw/Strawberry___healthy/4cdda951-679e-430d-b7a3-f8cc6c95a5b8___RS_HL 1726.JPG',\n",
       " '../data/raw/Strawberry___healthy/5b952db1-52ab-454d-9f13-8a404a7449ba___RS_HL 4693.JPG',\n",
       " '../data/raw/Strawberry___healthy/1cb7b773-be85-4c1e-88b7-9a598089ebb0___RS_HL 1805.JPG',\n",
       " '../data/raw/Strawberry___healthy/941bf773-f9a3-4807-b742-496cc43913f4___RS_HL 4851.JPG',\n",
       " '../data/raw/Strawberry___healthy/335c0d85-bf45-4cd0-9135-81447bab4ff9___RS_HL 1944.JPG',\n",
       " '../data/raw/Strawberry___healthy/e3a17b75-038a-440e-8a05-ec81625091e7___RS_HL 4674.JPG',\n",
       " '../data/raw/Strawberry___healthy/2e738c4c-8d5c-4c1d-91ad-52dfa3e4ec1f___RS_HL 1768.JPG',\n",
       " '../data/raw/Strawberry___healthy/373b483b-022f-4de0-8907-2b485aca6184___RS_HL 4491.JPG',\n",
       " '../data/raw/Strawberry___healthy/ce4aa9cf-c499-4ed5-bdfd-f7be230b3bfb___RS_HL 4523.JPG',\n",
       " '../data/raw/Strawberry___healthy/e54205fc-ef01-4553-8305-0067ebf3bdc5___RS_HL 1839.JPG',\n",
       " '../data/raw/Strawberry___healthy/a845e8b5-bd7a-4438-8936-e3191d2d3ba5___RS_HL 1653.JPG',\n",
       " '../data/raw/Strawberry___healthy/26fb9410-2d79-482d-bff7-a6210dda4c07___RS_HL 1746.JPG',\n",
       " '../data/raw/Strawberry___healthy/5e5147e3-ba21-4afa-a32b-bb3453c72944___RS_HL 4450.JPG',\n",
       " '../data/raw/Strawberry___healthy/4f288160-7226-425e-afe7-826fb19af650___RS_HL 2043.JPG',\n",
       " '../data/raw/Strawberry___healthy/6baab67a-a32d-4b53-a966-feea0ccdef77___RS_HL 4712.JPG',\n",
       " '../data/raw/Strawberry___healthy/f0d8204d-af72-4027-82d3-a91a889a6b7d___RS_HL 4694.JPG',\n",
       " '../data/raw/Strawberry___healthy/941b9454-80e0-44c5-b1ca-d0b16527d2b7___RS_HL 4811.JPG',\n",
       " '../data/raw/Strawberry___healthy/b0396a65-963b-4a4b-86eb-70f46b38752e___RS_HL 4678.JPG',\n",
       " '../data/raw/Strawberry___healthy/273a7a9e-18be-4b6a-976a-fa5ffd69b731___RS_HL 4366.JPG',\n",
       " '../data/raw/Strawberry___healthy/b9612bf8-67ad-4f8a-96f3-102869f7887f___RS_HL 1931.JPG',\n",
       " '../data/raw/Strawberry___healthy/9eca6d2b-a161-4a6c-9daa-6f875bb47de1___RS_HL 2231.JPG',\n",
       " '../data/raw/Strawberry___healthy/d9c894d9-d2e8-412f-85fa-f0a416185931___RS_HL 4546.JPG',\n",
       " '../data/raw/Strawberry___healthy/f058c607-09ac-4fe6-ad0f-c957d48bb0ed___RS_HL 4348.JPG',\n",
       " '../data/raw/Strawberry___healthy/74a16208-9fdf-4fc9-bc45-c400f8ca748f___RS_HL 1861.JPG',\n",
       " '../data/raw/Strawberry___healthy/1d184771-bec1-4c8b-9391-d2a506b53e60___RS_HL 4704.JPG',\n",
       " '../data/raw/Strawberry___healthy/e5d59ce2-d47d-45f9-b3e0-03f5797c981e___RS_HL 4372.JPG',\n",
       " '../data/raw/Strawberry___healthy/02caa98d-1c74-43b3-b3ee-e8492998f82a___RS_HL 2090.JPG',\n",
       " '../data/raw/Strawberry___healthy/c7d30b99-4e9d-4615-b1f4-bcc93ecb7919___RS_HL 4778.JPG',\n",
       " '../data/raw/Strawberry___healthy/2e89b7fa-24b0-48dd-8f2e-ec9c13b93b05___RS_HL 1992.JPG',\n",
       " '../data/raw/Strawberry___healthy/03fe6806-ca61-46c1-8889-45e40228c168___RS_HL 4632.JPG',\n",
       " '../data/raw/Strawberry___healthy/99140b91-93f1-4ccf-9761-e0370717baf6___RS_HL 4552.JPG',\n",
       " '../data/raw/Strawberry___healthy/88c5dda1-d947-44bc-a3e7-a954070ab87f___RS_HL 4731.JPG',\n",
       " '../data/raw/Strawberry___healthy/23f8718a-9556-4ad1-a9a0-4c895ee5a91b___RS_HL 2226.JPG',\n",
       " '../data/raw/Strawberry___healthy/b7f04b0d-c128-42f8-a0ea-2d8f24149ffd___RS_HL 1681.JPG',\n",
       " '../data/raw/Strawberry___healthy/d1aee44a-b6bb-45b9-b7b6-5d553add8fd1___RS_HL 2163.JPG',\n",
       " '../data/raw/Strawberry___healthy/00532378-bfb2-4d3b-8b19-903ecc085624___RS_HL 2021.JPG',\n",
       " '../data/raw/Strawberry___healthy/958d4cb9-4f30-4418-80f6-124d221fb3c1___RS_HL 1764.JPG',\n",
       " '../data/raw/Strawberry___healthy/59f517ea-070a-4d91-97bb-936c620bbeb5___RS_HL 4583.JPG',\n",
       " '../data/raw/Strawberry___healthy/72e7a6fb-c71a-437d-82f0-2032a43998c7___RS_HL 1702.JPG',\n",
       " '../data/raw/Strawberry___healthy/8a540a98-3e4e-44a9-b1c3-990772c57985___RS_HL 4597.JPG',\n",
       " '../data/raw/Strawberry___healthy/675d6618-29f2-41e7-8b0b-d7facdf9cd67___RS_HL 2105.JPG',\n",
       " '../data/raw/Strawberry___healthy/c2052c26-d4f7-4f0a-845b-5e25cc2f6e48___RS_HL 4409.JPG',\n",
       " '../data/raw/Strawberry___healthy/c9c97696-27f4-4f57-939f-b537ece8e668___RS_HL 4562.JPG',\n",
       " '../data/raw/Strawberry___healthy/d7ba4287-1140-40e5-9c9b-d596dbab6198___RS_HL 4370.JPG',\n",
       " '../data/raw/Strawberry___healthy/3b744062-8dff-416e-9349-3bae2aae5eaa___RS_HL 4345.JPG',\n",
       " '../data/raw/Strawberry___healthy/a1eddbee-09c4-4671-b079-b9ad9cdfbd46___RS_HL 1667.JPG',\n",
       " '../data/raw/Strawberry___healthy/866c2369-534c-4963-8703-b5e0257fea81___RS_HL 4703.JPG',\n",
       " '../data/raw/Strawberry___healthy/e0a7a770-3f85-4cad-a62b-0633dd7a8265___RS_HL 4594.JPG',\n",
       " '../data/raw/Strawberry___healthy/38292ca8-c7fe-4ba5-837f-c98c714ab099___RS_HL 4608.JPG',\n",
       " '../data/raw/Strawberry___healthy/f58ee24e-67e0-4883-a6b9-9591634e535c___RS_HL 1949.JPG',\n",
       " '../data/raw/Strawberry___healthy/60856065-b1f0-45bb-aa6f-e2d569990834___RS_HL 4355.JPG',\n",
       " '../data/raw/Strawberry___healthy/efbbf9ee-e60d-4522-9711-f92ae6e7adea___RS_HL 4586.JPG',\n",
       " '../data/raw/Strawberry___healthy/3fef560f-5660-4e47-bc07-7a847ceb6a3f___RS_HL 4740.JPG',\n",
       " '../data/raw/Strawberry___healthy/a8d7bf99-7163-49e1-a1b7-45ce12b4dc6c___RS_HL 4526.JPG',\n",
       " '../data/raw/Strawberry___healthy/01e591c9-e3e7-4edc-8211-13081f4d5e7a___RS_HL 1979.JPG',\n",
       " '../data/raw/Strawberry___healthy/02808b3e-ae88-4259-9b2c-f9096db336e4___RS_HL 1827.JPG',\n",
       " '../data/raw/Strawberry___healthy/f755c827-8e42-4cc1-a8fa-30951a9fc91c___RS_HL 2095.JPG',\n",
       " '../data/raw/Strawberry___healthy/672fc7b7-9bb6-4966-9240-41214be74253___RS_HL 1716.JPG',\n",
       " '../data/raw/Strawberry___healthy/5cb2ec10-af9e-4c1b-baa5-e28b700522f0___RS_HL 4413.JPG',\n",
       " '../data/raw/Strawberry___healthy/fb8db167-dda8-4bda-99a5-d7b688b14e89___RS_HL 4442.JPG',\n",
       " '../data/raw/Strawberry___healthy/d3ff051d-c299-4df2-b3fd-ba515c062385___RS_HL 1769.JPG',\n",
       " '../data/raw/Strawberry___healthy/d3da328c-a91f-4fcc-93fb-2c58baacdeeb___RS_HL 1741.JPG',\n",
       " '../data/raw/Strawberry___healthy/bf79e7f8-5b1e-4acd-b4aa-7c023fb284e1___RS_HL 4699.JPG',\n",
       " '../data/raw/Strawberry___healthy/83f32f06-ec1e-4dac-b3dc-b51f15133fb5___RS_HL 1980.JPG',\n",
       " '../data/raw/Strawberry___healthy/8fc17609-45eb-4a9e-9b71-7a7407936c5f___RS_HL 4394.JPG',\n",
       " '../data/raw/Strawberry___healthy/eabd6c29-7543-4274-95f3-78e3e80f45c9___RS_HL 4735.JPG',\n",
       " '../data/raw/Strawberry___healthy/f90cc425-5b46-4abd-b58a-11306f471542___RS_HL 4337.JPG',\n",
       " '../data/raw/Strawberry___healthy/3aca733d-6d49-42d2-bcc2-fa288d126411___RS_HL 2223.JPG',\n",
       " '../data/raw/Strawberry___healthy/f1b5ec9d-ed27-4e68-8316-63dc58ef35ab___RS_HL 1693.JPG',\n",
       " '../data/raw/Strawberry___healthy/7673a7d4-c27f-44c5-96a8-c9f133878ba0___RS_HL 2240.JPG',\n",
       " '../data/raw/Strawberry___healthy/97af749a-c269-4ec5-92b1-6e7384dba46b___RS_HL 4344.JPG',\n",
       " '../data/raw/Strawberry___healthy/bf4fea74-072d-4420-84b7-59c6a8edd677___RS_HL 4680.JPG',\n",
       " '../data/raw/Strawberry___healthy/288fa688-aee7-445a-9a90-427969c7fdb4___RS_HL 4456.JPG',\n",
       " '../data/raw/Strawberry___healthy/577164f8-3653-4347-a51e-ac0f67ef5a43___RS_HL 2158.JPG',\n",
       " '../data/raw/Strawberry___healthy/c0cd3f14-d2b4-43b5-a81d-0dc60767e52b___RS_HL 4539.JPG',\n",
       " '../data/raw/Strawberry___healthy/5275777b-d89b-47b1-9dc6-15c832167a41___RS_HL 4408.JPG',\n",
       " '../data/raw/Strawberry___healthy/c051460a-5c1c-4a9e-8921-0150225545fa___RS_HL 4849.JPG',\n",
       " '../data/raw/Strawberry___healthy/f3e00f77-450d-4271-be54-5884aaa8d64e___RS_HL 2241.JPG',\n",
       " '../data/raw/Strawberry___healthy/7e02bc12-a026-4aff-88b1-78dcd0a97fa1___RS_HL 1889.JPG',\n",
       " '../data/raw/Strawberry___healthy/39d0dfab-0fd3-49e0-8bef-702c97490655___RS_HL 2001.JPG',\n",
       " '../data/raw/Strawberry___healthy/734ec913-c493-4d86-bf32-d52f6e063218___RS_HL 4859.JPG',\n",
       " '../data/raw/Strawberry___healthy/672451d5-6018-4abf-87f8-7b1c0b066136___RS_HL 1902.JPG',\n",
       " '../data/raw/Strawberry___healthy/640bb1de-c435-49a6-9784-30c1e0770a26___RS_HL 1685.JPG',\n",
       " '../data/raw/Strawberry___healthy/ff7ccace-1453-44e8-8580-3263c2952ea8___RS_HL 2006.JPG',\n",
       " '../data/raw/Strawberry___healthy/89e133b4-9f70-4711-a14e-2535ae3a00ee___RS_HL 4792.JPG',\n",
       " '../data/raw/Strawberry___healthy/58265359-4fa0-458c-9c5f-5774d94b3088___RS_HL 1737.JPG',\n",
       " '../data/raw/Strawberry___healthy/93e51b7e-3a41-40e3-9790-c5eeb86a4d61___RS_HL 4706.JPG',\n",
       " '../data/raw/Strawberry___healthy/411e3372-e40e-44ed-aa47-972afabd15f7___RS_HL 2225.JPG',\n",
       " '../data/raw/Strawberry___healthy/a34f4043-6213-42b5-a837-b372ef04c959___RS_HL 4454.JPG',\n",
       " '../data/raw/Strawberry___healthy/f13a9b3f-74e0-4e57-8932-4b80f9581c85___RS_HL 4402.JPG',\n",
       " '../data/raw/Strawberry___healthy/23bce3ab-97d8-43a9-b38b-9e4e6fa57807___RS_HL 1829.JPG',\n",
       " '../data/raw/Strawberry___healthy/dc5b5a9e-c144-49d7-99ca-b53a818f83d8___RS_HL 4744.JPG',\n",
       " '../data/raw/Strawberry___healthy/aa47a5b4-789e-43ff-acd0-70bef9f8a365___RS_HL 4564.JPG',\n",
       " '../data/raw/Strawberry___healthy/41ceda85-676a-412a-bc22-324700edc9ea___RS_HL 2237.JPG',\n",
       " '../data/raw/Strawberry___healthy/6e3bbd66-5fb8-4cef-a84f-ac093bb96b1f___RS_HL 4856.JPG',\n",
       " '../data/raw/Strawberry___healthy/d454c3f8-ca2e-4935-9c09-3b6a480ac097___RS_HL 1870.JPG',\n",
       " '../data/raw/Strawberry___healthy/c00b240e-63b9-4b16-b7ba-958f0be960e4___RS_HL 1684.JPG',\n",
       " '../data/raw/Strawberry___healthy/bbd6eeef-95ad-4277-85a4-243de6d117d6___RS_HL 4518.JPG',\n",
       " '../data/raw/Strawberry___healthy/1245a96d-9a84-402d-9dde-da8c88bf7d74___RS_HL 4474.JPG',\n",
       " '../data/raw/Strawberry___healthy/65759427-df44-4ec4-8598-c8bde2d431d7___RS_HL 4424.JPG',\n",
       " '../data/raw/Strawberry___healthy/85726470-e247-4d82-89d1-f6adad570519___RS_HL 2150.JPG',\n",
       " '../data/raw/Strawberry___healthy/29a3e391-c98b-4fff-a03e-41257d067884___RS_HL 2057.JPG',\n",
       " '../data/raw/Strawberry___healthy/30baaa11-cdf2-4af0-9e96-cf6b688b99c0___RS_HL 1786.JPG',\n",
       " '../data/raw/Strawberry___healthy/9f807512-4c4c-46f9-a9d1-66f398f29bfb___RS_HL 1873.JPG',\n",
       " '../data/raw/Strawberry___healthy/a386a7c2-9a92-4dd3-bffc-713d3a545b1f___RS_HL 1848.JPG',\n",
       " '../data/raw/Strawberry___healthy/e25be174-f4b9-453e-9287-faae83c62ff4___RS_HL 2025.JPG',\n",
       " '../data/raw/Strawberry___healthy/53f62244-534b-49c3-ac2d-c26a1d279b97___RS_HL 2151.JPG',\n",
       " '../data/raw/Strawberry___healthy/d069637f-101f-4e6a-84d3-5f646a6bbff7___RS_HL 2050.JPG',\n",
       " '../data/raw/Strawberry___healthy/8f558908-aa1b-4a86-855a-5094c2392e5a___RS_HL 1973.JPG',\n",
       " '../data/raw/Strawberry___healthy/daf99263-f38b-4aa6-ac5e-c82a10065773___RS_HL 1800.JPG',\n",
       " '../data/raw/Strawberry___healthy/714e3e69-0d2e-44d1-973f-dcf8c578ce4d___RS_HL 1765.JPG',\n",
       " '../data/raw/Strawberry___healthy/fb1750de-1250-4bf5-806b-ff01ce24e40f___RS_HL 4354.JPG',\n",
       " '../data/raw/Strawberry___healthy/7e760452-b0a5-48b0-8fb8-8e8a4f84c4e1___RS_HL 4463.JPG',\n",
       " '../data/raw/Strawberry___healthy/f04c2b50-150f-4e55-ae60-6cbff469eda1___RS_HL 4772.JPG',\n",
       " '../data/raw/Strawberry___healthy/179cce8d-751b-454d-978f-dde2c84a1231___RS_HL 2060.JPG',\n",
       " '../data/raw/Strawberry___healthy/19a8e3cb-c287-4871-9cb3-23e17fd5bf27___RS_HL 2160.JPG',\n",
       " '../data/raw/Strawberry___healthy/61c8f6ba-9074-44c7-852b-4f34032a6b5e___RS_HL 1721.JPG',\n",
       " '../data/raw/Strawberry___healthy/2dbd44d9-8c98-45ac-92a3-e6e65aac4dd5___RS_HL 4818.JPG',\n",
       " '../data/raw/Strawberry___healthy/2dd9d66d-56e4-4c31-a782-502526621455___RS_HL 2010.JPG',\n",
       " '../data/raw/Strawberry___healthy/6a575843-9563-4d8d-a54a-0512c08ed6e3___RS_HL 4398.JPG',\n",
       " '../data/raw/Strawberry___healthy/8c81db30-73de-4353-9eed-0afded302fe4___RS_HL 1830.JPG',\n",
       " '../data/raw/Strawberry___healthy/6c08a905-65f4-4733-9e6a-081844b9f9a7___RS_HL 2029.JPG',\n",
       " '../data/raw/Strawberry___healthy/786744bd-7252-4dec-b86a-b2ecfaa0b33f___RS_HL 4874.JPG',\n",
       " '../data/raw/Strawberry___healthy/2df352eb-8c72-4a03-9cd6-b94e38e41bb3___RS_HL 1967.JPG',\n",
       " '../data/raw/Strawberry___healthy/b8e9ed27-8e37-4214-9206-f8c0ef21cf4d___RS_HL 4847.JPG',\n",
       " '../data/raw/Strawberry___healthy/60e9ea3a-fc7a-44ba-8875-1917ac056e5c___RS_HL 4381.JPG',\n",
       " '../data/raw/Strawberry___healthy/6cdf4322-10d1-40e1-96d8-6db10ae52144___RS_HL 1672.JPG',\n",
       " '../data/raw/Strawberry___healthy/f36cfb30-4d03-46e2-8899-d372bf05212f___RS_HL 2112.JPG',\n",
       " '../data/raw/Strawberry___healthy/db14ea51-547d-4e87-a7c4-f6e1a57c35c2___RS_HL 4502.JPG',\n",
       " '../data/raw/Strawberry___healthy/5455791a-eeab-4665-841c-a38e68058b54___RS_HL 2135.JPG',\n",
       " '../data/raw/Strawberry___healthy/13c61e01-c2ee-4ab2-9621-22c6d7d4cdf1___RS_HL 4837.JPG',\n",
       " '../data/raw/Strawberry___healthy/b4db91ed-57d5-4498-80de-c664a279c7d1___RS_HL 1970.JPG',\n",
       " '../data/raw/Strawberry___healthy/a5f3534e-b953-4925-abfa-999eeb8aaf64___RS_HL 4845.JPG',\n",
       " '../data/raw/Strawberry___healthy/00e9a277-ca5e-4350-95ce-8b2918b69fb9___RS_HL 4667.JPG',\n",
       " '../data/raw/Strawberry___healthy/95330a25-8603-4af7-8be9-0d8f7a7241a0___RS_HL 1643.JPG',\n",
       " '../data/raw/Strawberry___healthy/788c9f33-f3aa-4cd6-af6b-000be785352c___RS_HL 4405.JPG',\n",
       " '../data/raw/Strawberry___healthy/bf22767d-2690-40b4-8991-b49ccd749cdc___RS_HL 2023.JPG',\n",
       " '../data/raw/Strawberry___healthy/16cb49b2-8d1b-4a66-95fa-9487aa0ba8f1___RS_HL 1665.JPG',\n",
       " '../data/raw/Strawberry___healthy/fc93155b-55bc-4cab-8807-e5cc6f896384___RS_HL 2082.JPG',\n",
       " '../data/raw/Strawberry___healthy/a6badbd1-59ff-4476-8b56-e2a61e993918___RS_HL 4603.JPG',\n",
       " '../data/raw/Strawberry___healthy/4c5da64b-2f39-4c84-9130-0ff7b8fcda7a___RS_HL 1662.JPG',\n",
       " '../data/raw/Strawberry___healthy/11ffeee8-fc5b-416a-bc86-c7a66e868181___RS_HL 2138.JPG',\n",
       " '../data/raw/Strawberry___healthy/0b444634-b557-45f4-a68a-8e9e38cd6683___RS_HL 2184.JPG',\n",
       " '../data/raw/Strawberry___healthy/ac5bf646-1586-4766-9583-ef6eb8f7b4a6___RS_HL 4774.JPG',\n",
       " '../data/raw/Strawberry___healthy/6e8627f0-d19e-46c2-876a-d36c5c57c25a___RS_HL 4503.JPG',\n",
       " '../data/raw/Strawberry___healthy/3303d3bf-c26b-47b9-ade2-b51348eaf7a1___RS_HL 4465.JPG',\n",
       " '../data/raw/Strawberry___healthy/27c2aaa4-de4b-4fb1-ba8d-14d4b24b2afa___RS_HL 4665.JPG',\n",
       " '../data/raw/Strawberry___healthy/24077dde-8ba8-4a61-ba95-92958347da79___RS_HL 2233.JPG',\n",
       " '../data/raw/Strawberry___healthy/975a4f22-0c55-4d2a-a4a2-e8686acc9897___RS_HL 1799.JPG',\n",
       " '../data/raw/Strawberry___healthy/a1ff6777-0146-4138-befb-1f0afad46834___RS_HL 2052.JPG',\n",
       " '../data/raw/Strawberry___healthy/43c3ba78-39d0-49e8-92f1-34b747f9d1ef___RS_HL 4411.JPG',\n",
       " '../data/raw/Strawberry___healthy/d1f8f26c-904b-4c19-8849-bc0d79b084de___RS_HL 2179.JPG',\n",
       " '../data/raw/Strawberry___healthy/2acb16d1-598e-4ee1-9b54-eecfba7d4fc3___RS_HL 1754.JPG',\n",
       " '../data/raw/Strawberry___healthy/60646de2-333f-40b6-bdf9-f0d1d0e91271___RS_HL 4795.JPG',\n",
       " '../data/raw/Strawberry___healthy/36a5809f-afaf-4c09-ab0c-023ce4fb2ba6___RS_HL 4592.JPG',\n",
       " '../data/raw/Strawberry___healthy/f3b01730-9365-4b94-a477-9b95d959ccd8___RS_HL 4555.JPG',\n",
       " '../data/raw/Strawberry___healthy/7a0c229c-035e-4c92-b2d4-fb3ed1f1d84a___RS_HL 2242.JPG',\n",
       " '../data/raw/Strawberry___healthy/1faaac0a-ea12-4085-af35-ab8b70bbf60a___RS_HL 4725.JPG',\n",
       " '../data/raw/Strawberry___healthy/0e527d62-de6c-4ab5-bb49-f7a6686d207b___RS_HL 4808.JPG',\n",
       " '../data/raw/Strawberry___healthy/1eb7dc9e-fbbb-4a62-8a71-eed5f2cb6845___RS_HL 4614.JPG',\n",
       " '../data/raw/Strawberry___healthy/4b9e296e-f995-4632-976b-7cd15862eea0___RS_HL 1777.JPG',\n",
       " '../data/raw/Strawberry___healthy/096ce0d4-c5ef-4b66-b20f-f9ff62d09f48___RS_HL 1879.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e9d15f1c-6ff9-4220-918d-96a726279081___RS_L.Scorch 9960.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ad08e822-1ff0-48e9-9ff3-cc8d0b53fc70___RS_L.Scorch 1288.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bb913cd7-ee71-4ac1-9b04-1417fcd67456___RS_L.Scorch 1274.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1740c9dc-3dd7-4053-b77c-41dff00a0d59___RS_L.Scorch 0053.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/76e3f721-2ce3-4f3d-a533-72c3716540c6___RS_L.Scorch 1310.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d6a52691-5da2-47dd-b4f3-bd41a0274e94___RS_L.Scorch 1116.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d2d6b4e7-806e-43ce-bf1b-d73c887c7bc3___RS_L.Scorch 0059.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/11c7dacf-e6f4-4a31-a89b-e0d221eefdbf___RS_L.Scorch 1032.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/13d5364a-7ac5-4c5f-9cf1-23c64499a620___RS_L.Scorch 1342.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8e64ad2f-7413-4c5b-b80a-175bdbc29aa1___RS_L.Scorch 1616.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3cefe53f-b1ff-4d34-9ecb-825f260e720d___RS_L.Scorch 1436.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/11081be7-ebf9-44b0-aa58-818ccba6f74c___RS_L.Scorch 1573.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0dda76f4-d38e-4547-a341-546e84d72346___RS_L.Scorch 9973.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b8337539-4a9f-4df5-be0d-266a7b792cb2___RS_L.Scorch 0016.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dfd9d830-d639-4ead-a2ec-dddded0878e8___RS_L.Scorch 0883.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4d7a3636-670f-47cc-b40b-c88b7783f60f___RS_L.Scorch 1383.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5ddbd472-b511-408e-b1cc-8ea77ce93c8c___RS_L.Scorch 1077.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7cf5daf3-540a-4451-bbad-3721d2290c1c___RS_L.Scorch 0830.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2ce66a24-6c21-433c-a6ff-be65597b7bed___RS_L.Scorch 0140.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5b19c65d-67a1-439e-b699-bb41ed610fba___RS_L.Scorch 1601.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/02d53294-aa1c-4a33-8bdc-fd987e700305___RS_L.Scorch 9994.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c76aba29-28a3-43d4-949f-ea8b75f9c8f3___RS_L.Scorch 0087.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/247833b5-b655-418d-80c8-72fa4889dc5b___RS_L.Scorch 9978.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f98e850b-3a8b-4a11-be07-6b4aa14e56a9___RS_L.Scorch 1048.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9376ef24-0403-42bb-910d-60c3eda976ec___RS_L.Scorch 1105.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ad48a0f5-3036-4d59-9d9b-27a5ea752131___RS_L.Scorch 9964.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/796d0728-f68f-42c4-8b06-277b9ba2fd78___RS_L.Scorch 1265.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6c1b8a7b-f7c3-4c9d-b04d-c9885df5ed31___RS_L.Scorch 0818.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ccacc967-db99-4d29-a7cf-505cfc497eaa___RS_L.Scorch 1572.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4c817438-77b9-4b33-b272-6be7a49863de___RS_L.Scorch 1570.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d06a9be8-2cfc-464c-8b2b-20f5442ea8fb___RS_L.Scorch 9963.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/220e00d7-57f3-4aa5-b481-9652d44e1642___RS_L.Scorch 1184.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1e1544ee-5642-4bef-847e-f961db4580ae___RS_L.Scorch 0038.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e917ee00-f360-4c1c-8833-aeac83f55857___RS_L.Scorch 0873.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4ab86c25-4b68-4e73-9f06-aaae4bef79b4___RS_L.Scorch 1040.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/320d687e-0c4f-4ecc-bf4c-b2d1aaf2fd20___RS_L.Scorch 0855.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e84ee8ed-5a7b-46e7-ae90-5b9571d789dc___RS_L.Scorch 9981.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e455feaf-e521-481c-ba4e-e899fb0d49c5___RS_L.Scorch 1159.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8408f04f-8155-403e-8906-f446f4d40acb___RS_L.Scorch 1476.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d7d929e4-49c8-4bac-88ee-baf0447e2f3d___RS_L.Scorch 1525.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8390e1a5-7962-4523-a31e-e4928c2e17bc___RS_L.Scorch 0877.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8e471027-aaa7-4a8a-aba0-217ca766f297___RS_L.Scorch 0084.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/94b00c87-de7a-4a53-876d-48cb43106f72___RS_L.Scorch 0143.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5ab90f76-2584-402a-a8d2-661c0ebb354f___RS_L.Scorch 9966.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a74e684d-f2f1-4e0d-aea2-f68054714a4c___RS_L.Scorch 1537.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b56e74f4-6cdf-4476-8759-817fa8e58439___RS_L.Scorch 0117.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bcd62227-c477-4cbf-9a55-b6c048f8bd0a___RS_L.Scorch 1221.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/14f6be82-3dbe-4194-a217-cb0038cf6f2b___RS_L.Scorch 0961.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dcce349c-539c-43b7-8062-bd76b04fec67___RS_L.Scorch 1041.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/923d8467-ed9a-488d-b5b6-9cd0468ef321___RS_L.Scorch 0971.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e6a59827-6b18-4470-835b-e1052b284007___RS_L.Scorch 0035.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bcccdd25-1271-4d6e-b80a-e2af8f487fb5___RS_L.Scorch 1535.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/36307818-823a-4874-a556-c2c874314753___RS_L.Scorch 9947.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2cc638a0-2dbd-4287-b9bb-35402baf0f4b___RS_L.Scorch 1493.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e9f7de15-8ed4-4c7b-9c04-223b0e00de81___RS_L.Scorch 9904.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f107935e-2a68-4365-a8b4-a9b9bf9724b1___RS_L.Scorch 0114.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ff4f4171-06cd-47c3-af7f-87f522a33ab1___RS_L.Scorch 1531.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bfde826a-4ff5-49e0-b6b8-f3a14f4c7eeb___RS_L.Scorch 9990.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a76aa051-be2d-4913-a50b-3cec617019fe___RS_L.Scorch 0788.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5448ed57-944c-47d5-b35a-d1f85dc5bb25___RS_L.Scorch 1475.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/98208610-36ea-4082-97a2-9c2c8a5dd00f___RS_L.Scorch 0095.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/87d9330d-9284-47af-8210-ad6f06a7f64d___RS_L.Scorch 1232.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a34c4018-c690-4e5f-8e3a-385be622f9d4___RS_L.Scorch 1060.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7796c398-c9eb-4d77-bd40-ef5fbf7a424a___RS_L.Scorch 1005.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/427ab6dd-58e8-4c02-8908-88ab2101cbb5___RS_L.Scorch 0169.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ea5d5ed5-c066-4f78-a633-585349910c47___RS_L.Scorch 1151.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3b44d2b5-fc5a-497c-92fb-1068e323c67e___RS_L.Scorch 1569.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/35afe275-3d26-4f6c-9c35-2a8c76fad8eb___RS_L.Scorch 1262.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2d90a8b3-1a11-4c72-9334-42fb50862f6d___RS_L.Scorch 9992.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3cde8e35-98a2-4f84-b96d-7071aa05c62d___RS_L.Scorch 0008.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e2a23277-7bc0-47cf-9a90-8459d2c3f70d___RS_L.Scorch 0037.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d63a4b03-0317-4385-9fa4-dd406158df53___RS_L.Scorch 0963.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/11e01399-e741-49de-b59c-aa2c2f1a0f66___RS_L.Scorch 1603.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4e6bb3fe-cc04-4ebd-affa-0a48d9a5868d___RS_L.Scorch 1546.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bbf5cb98-385e-42b0-b3c1-1762ab684daa___RS_L.Scorch 1034.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ae29ceb2-6460-49dd-9590-23012292ddee___RS_L.Scorch 0879.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9b496567-3855-496d-8113-d4947f93c300___RS_L.Scorch 0876.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/43c14761-a95f-4732-b5cd-5c2d4d4940de___RS_L.Scorch 9900.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b1ead01a-3f1c-4f9c-a91e-0ddaa45112a4___RS_L.Scorch 0783.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/08d4f54d-2676-4ce4-8f76-ca89b7b199fe___RS_L.Scorch 1316.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f8a87615-21a8-4511-9c67-e82394e55329___RS_L.Scorch 1341.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/90997894-f4c1-4489-860c-bc0a0662130a___RS_L.Scorch 0112.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/313768fa-601a-47f5-a47d-9bb9c2d1b97e___RS_L.Scorch 1251.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7bf3231a-e7c5-4293-99fc-b61f90cd2a84___RS_L.Scorch 1433.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/638c2ec7-0381-4b70-8b84-8b270e70cd87___RS_L.Scorch 1085.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4ea8bbcd-97f3-4703-8dd6-9352400212c8___RS_L.Scorch 1083.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7a0d3bb1-3b92-484a-b63c-07418451a42a___RS_L.Scorch 1422.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/47a98502-4839-44cd-b855-fd4d3f9f1810___RS_L.Scorch 0120.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8b11b530-7a05-4fbd-81a4-a057cbf44d9d___RS_L.Scorch 0001.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e73b74f2-bd43-4ce6-9cb5-ac8846d6d524___RS_L.Scorch 1382.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a8a746ee-d8c4-48c6-8601-cd72e7bbf235___RS_L.Scorch 9930.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6e5e0267-fdff-4f09-acdb-ae08f56f4412___RS_L.Scorch 0822.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f7ee8706-dac1-41c5-9db6-997277cec8ee___RS_L.Scorch 1402.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e4d9dc76-f555-4ad9-a7e5-33a21ae5e6cf___RS_L.Scorch 9924.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7acfc5d0-b17e-4d35-909c-f9b0320b01c1___RS_L.Scorch 9921.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f631beb6-d7fd-452b-8581-8a1f51bd0fce___RS_L.Scorch 1186.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e7742c1f-7ea0-40a1-834e-eea9cf538641___RS_L.Scorch 0076.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8573df4b-218b-42ad-9d39-36e18dfc7520___RS_L.Scorch 1350.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2e717101-b9db-47c3-9b67-8dc4cf08c88a___RS_L.Scorch 0952.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1f7f3e65-fb9f-4a05-8304-26850067d617___RS_L.Scorch 1536.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b2f174f5-6ab4-46db-adbc-a392bebe3f1f___RS_L.Scorch 0926.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/fb6de9dc-fb0b-4c43-a1e2-a37436902e62___RS_L.Scorch 1473.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/516b081c-77ec-40bc-a6c1-0a1c18a353e2___RS_L.Scorch 1292.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/115b843f-5c32-48dd-9602-b8fa5a3c7b29___RS_L.Scorch 9986.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/68339bd8-5a90-46c3-b087-0c0584cd706b___RS_L.Scorch 1584.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/62114343-a924-445c-9a9a-9103fc797414___RS_L.Scorch 0116.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/064044ca-4bb3-485f-80a5-9cea1b26d453___RS_L.Scorch 1075.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8118be5a-94c0-4ca9-ae78-ad7a0ccbdc8a___RS_L.Scorch 1434.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2ccd8529-b99b-4ada-81c8-f9ae6e25cf3e___RS_L.Scorch 1014.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2cecdc8a-64a9-4aef-b9ec-c657b4b0ba51___RS_L.Scorch 1064.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/73c28c4a-6899-404e-a107-39445bc7be09___RS_L.Scorch 1091.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/91795b04-c65c-4a0e-99d0-793b797ed3c4___RS_L.Scorch 9971.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8e704613-f271-417f-8d8a-8b510a327a77___RS_L.Scorch 1398.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/954f4764-c75e-41e3-a7d7-ae2b3aaba672___RS_L.Scorch 1291.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0e30a013-04c1-4936-b8b3-1eeaa65cfaac___RS_L.Scorch 1559.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/64dcb8c1-3f29-46be-98b4-a01121dd9223___RS_L.Scorch 1243.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/21414b15-f3a6-4f37-af6c-7374939ecae9___RS_L.Scorch 0015.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e80d81f2-f9b6-480c-959e-9a169d42acd0___RS_L.Scorch 1160.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8830892d-8a46-498a-bc52-2da674bf1288___RS_L.Scorch 1038.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e59efe9b-bfdf-4a4a-85e0-603a97c52083___RS_L.Scorch 0069.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dac0fb3a-21d0-471b-ba66-79a0e454a9c7___RS_L.Scorch 1353.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4f89ed72-202f-4357-b15e-246999c59d03___RS_L.Scorch 0004.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/19f6a0ac-b426-4297-a021-d4847c35e1eb___RS_L.Scorch 0007.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b10424c6-ae2c-40c4-a884-69b73cb65479___RS_L.Scorch 9897.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8f7f9c9f-37b9-45e0-9292-c6b69bdfb494___RS_L.Scorch 1575.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/aa8c2cc5-1f2b-448d-ad62-b70a82fdb911___RS_L.Scorch 0113.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/453330b0-1c32-4a61-b2e7-729f7f4acc16___RS_L.Scorch 0882.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/928fd5a7-5b1f-4fc0-af9f-4e159c4eee4a___RS_L.Scorch 1094.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/97178ff7-cfbe-4436-af6d-b7a2c693f895___RS_L.Scorch 0047.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/85b88dd2-5e69-4de4-9f41-e675c21427ac___RS_L.Scorch 1579.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/527ceb3f-b9b6-4c46-a12c-1cdafe4659b3___RS_L.Scorch 1328.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/79c2506c-98e7-4e81-8daa-cc30fcc19436___RS_L.Scorch 9925.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/98ddf858-e4f2-4a85-a98c-c1143d4c7394___RS_L.Scorch 9970.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/fdbd115d-4709-42eb-90ce-278a33fc3e7e___RS_L.Scorch 0079.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bdbe8588-e5e2-427b-8b56-834cb6212fe5___RS_L.Scorch 0085.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/508172b2-cb62-4d87-a88d-050fd2033f65___RS_L.Scorch 1582.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7ca809db-94a4-4c9c-a730-bf0b7e93855e___RS_L.Scorch 1169.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/925f7c20-9861-451c-a6fb-b165fc4b4aae___RS_L.Scorch 0802.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5f6e61a5-c917-43f7-978c-e1c7febde7c9___RS_L.Scorch 1290.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/13e6f9a3-0eff-4d81-a576-985cdc97872c___RS_L.Scorch 0165.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/87b91e6c-0e62-471a-a371-7eef8c6917df___RS_L.Scorch 1395.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f740e529-abd0-44e1-898e-92ee5afa16f1___RS_L.Scorch 1421.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b06eee95-6d12-4fab-bd1d-009dffd7aa12___RS_L.Scorch 0909.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/12e66247-e91e-4fef-b1c8-b08793d1e4ad___RS_L.Scorch 0025.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b41e624c-f0f2-480e-921a-126300ae4d35___RS_L.Scorch 0098.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c0be4fed-19c1-40ba-bb49-69b48a08015c___RS_L.Scorch 9985.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8baef81b-feab-4350-a22a-3bdded5963d7___RS_L.Scorch 1090.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d0615e5a-1eae-42b7-9b24-d02ceb86c16b___RS_L.Scorch 0131.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/89101976-5351-4b16-a471-19f2161f1e33___RS_L.Scorch 1222.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1388ef9c-6d08-4c30-8966-e43d7e299aa7___RS_L.Scorch 0071.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c2b86d13-b745-4bdf-9a69-a42c0f73785b___RS_L.Scorch 0988.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c50d2ba0-216b-4389-a828-13d03cd08263___RS_L.Scorch 1260.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8890b926-3a8e-434e-9775-bba40fb43839___RS_L.Scorch 1027.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/443c9ee2-4656-4752-bab8-650c4cd03d0a___RS_L.Scorch 1087.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70309299-2d91-4b08-b449-7c8bf8c477e9___RS_L.Scorch 1511.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/54351e43-8047-4fd4-a237-eb13f4c2b1d2___RS_L.Scorch 1574.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/eaf7b374-1627-486d-8592-ee2df00abf4a___RS_L.Scorch 1392.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/01647a51-6ee5-4686-8f60-26ebed68fe21___RS_L.Scorch 0130.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c1e1fa8d-3c89-460e-ac83-8ce693d742f4___RS_L.Scorch 0133.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e33a0518-4ca3-4ac9-b37e-0a2df961334a___RS_L.Scorch 1036.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c8290709-b8ae-4f1f-bfe0-852741b7cc00___RS_L.Scorch 1067.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e0565c85-5257-41b1-aae2-017c1c213200___RS_L.Scorch 1464.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/26fce8a7-3943-4a90-9486-000a6be3c436___RS_L.Scorch 1113.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b8d443df-aee7-4c88-98fc-0494b14d9b74___RS_L.Scorch 1033.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3e6e20d6-0cd0-45cb-9083-3d0c7b76322c___RS_L.Scorch 1072.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1233cddc-af2c-49e7-98c5-82036a9d5cc1___RS_L.Scorch 0904.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b234ecfc-d547-4ef5-b32f-d49a252b473c___RS_L.Scorch 0943.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9f0ba943-edb9-4b68-8e4b-7eb97833b131___RS_L.Scorch 1461.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/261b9095-4d7e-4546-8f92-dd6979b0fc22___RS_L.Scorch 1391.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4a2676a7-dedd-40c6-bd50-9012129ec708___RS_L.Scorch 1172.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cbec0dde-a302-4a91-b260-058ec4e7dfae___RS_L.Scorch 1201.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cc48387e-f674-4a82-b7ad-be5a0fb7aef0___RS_L.Scorch 1361.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bc12a3b9-5bf2-478c-96e3-78876119b5aa___RS_L.Scorch 1146.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6d1f6f9e-facb-4e5e-a0a9-31a30eb8bde4___RS_L.Scorch 0910.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b4f9293b-406b-4be2-8273-5329faac034c___RS_L.Scorch 0923.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1dba0c40-2f74-480a-896e-73cdfb752916___RS_L.Scorch 0984.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c6920663-efcc-433d-a429-d6ac89f686e9___RS_L.Scorch 1050.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/98ece15c-6146-4323-b461-ab1ced2e6daa___RS_L.Scorch 1162.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1a123c2f-bdb0-4b73-91ce-84284b211edd___RS_L.Scorch 1468.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e80d59db-8d78-497a-90fc-0b7fbdf48365___RS_L.Scorch 1273.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a3836f74-ad95-43c3-a3e2-f19a5c82be1c___RS_L.Scorch 0152.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/de72f767-861a-49cd-b0d1-0d6f4312c527___RS_L.Scorch 1294.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b192b930-a9ac-4e0a-be27-ccab87b6078a___RS_L.Scorch 1474.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ceb2c11a-034c-4651-ba12-d6919abe0078___RS_L.Scorch 0966.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/92ce3b48-8ed9-4faf-8cea-646c54570991___RS_L.Scorch 0990.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4ceb59e3-a16c-413e-965f-a148208bd495___RS_L.Scorch 0867.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/97e3b941-e828-47fa-87f8-338c626ba6a2___RS_L.Scorch 9916.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/12836245-bd1b-4032-b758-ac9128f5f31c___RS_L.Scorch 9946.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/87483aae-1c8c-47fc-af2d-512336395b2f___RS_L.Scorch 9967.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3b9454d1-8c70-4bca-ba30-60689208c217___RS_L.Scorch 1245.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1813404e-1833-498c-a939-b6a43baeb7df___RS_L.Scorch 0801.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6d2d1e82-01f1-451f-83d8-18fd6c7b9b95___RS_L.Scorch 1522.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c90232e6-83c7-4bd9-877f-c85445a1b2e9___RS_L.Scorch 1583.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f8f9559d-5dee-46ea-80ba-6a410cdf0715___RS_L.Scorch 0049.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e6cdee5b-7c34-483f-9a61-1bb9917d3586___RS_L.Scorch 1365.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6710e7b8-6dee-4b08-a924-280658020d41___RS_L.Scorch 0028.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/86268642-0704-4db7-a6fa-dd039ef14e0a___RS_L.Scorch 0050.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0c3c9e71-d339-4201-a32a-7a28f0556258___RS_L.Scorch 0930.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f72c3e45-ee30-480d-8682-1c216866cdae___RS_L.Scorch 0948.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c29a0891-d843-4861-8e09-47e4405d1176___RS_L.Scorch 1417.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/94d2c1fc-f862-456f-a7be-f09c27147f12___RS_L.Scorch 0031.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/97bcae7e-cee8-4537-aad3-2732e6f1ad88___RS_L.Scorch 1302.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7eb374a1-3a32-407d-85e0-1af8574dc3cd___RS_L.Scorch 0849.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/82e24bb0-1131-4411-a024-bfcaba541a4f___RS_L.Scorch 1440.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/43645b11-08c6-4c60-a19b-23ebaea1a7df___RS_L.Scorch 1337.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7d3c63ed-92ad-4809-b5d2-42d9adb94504___RS_L.Scorch 0893.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0a99c28d-18e3-4250-bac6-84a810438e0b___RS_L.Scorch 0154.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/69eaa143-9496-459f-a7fa-db5af81ba346___RS_L.Scorch 1022.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9579c6e9-8fbc-4b6c-8201-8bd3d4a6b64f___RS_L.Scorch 0779.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/71f8c314-ef34-4521-b575-e06f8455f449___RS_L.Scorch 9965.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1a01bc1f-7c24-41ba-a5e1-df558a755d25___RS_L.Scorch 1163.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/914e6b07-d7ff-4424-a0a9-7607e49a3715___RS_L.Scorch 0868.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5aad6970-758a-4edc-96fd-77237ec1cf50___RS_L.Scorch 1384.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/05227405-8c44-4ffd-a305-44a87d407013___RS_L.Scorch 1458.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1fdfa208-78e8-4c6f-8ff8-c70241b7354b___RS_L.Scorch 1283.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9bcdfc47-e072-4c08-9211-906177a22020___RS_L.Scorch 1179.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/afc3d229-2150-4ce3-8f3f-ef06ccb7a6e5___RS_L.Scorch 1153.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/eb9ca9ba-2989-46f8-9479-b2ed7c842e53___RS_L.Scorch 1544.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e1f777c9-fdd5-42f2-b7b2-e598b47f9e38___RS_L.Scorch 1362.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/93418610-b995-4e5d-9102-e44d8bc2afcc___RS_L.Scorch 1608.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3f6a523b-6fde-4fbd-8c6a-7cf84e96fd76___RS_L.Scorch 0968.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6abc96de-a85a-48c3-a821-5545e469122d___RS_L.Scorch 1386.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ef848b5e-8c3c-436c-bf74-c3c6008adaf6___RS_L.Scorch 1351.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a479bc72-8214-4b97-b9bd-60a9228762cf___RS_L.Scorch 1225.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ef86303b-ac0f-49bf-8cfb-77a3e0b6a1d1___RS_L.Scorch 0977.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e0133358-95d0-4989-93a5-aba0de775bc4___RS_L.Scorch 1332.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f78cfb53-cacc-4c0d-bccd-5ab3914e6351___RS_L.Scorch 9929.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4725a7d8-c7b3-4b3b-bf9d-f9a0e5f5a4da___RS_L.Scorch 1263.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c1637527-fb56-4ac5-a732-b56cc83d2bbb___RS_L.Scorch 1281.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1852f6bd-fe20-4b33-8f30-d6678465e8f2___RS_L.Scorch 1390.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5bf08fc4-3629-4ba8-9aef-9af9c4ea9f83___RS_L.Scorch 0840.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/73f3e57d-e47e-4469-811f-4067219a2093___RS_L.Scorch 1121.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6c63ab7c-68a8-4ff8-a001-f3e7cf16f722___RS_L.Scorch 9913.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2318c34f-6d18-421c-87ef-6c87cba47c79___RS_L.Scorch 9933.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/149dfa76-2e7b-4ed8-bbd3-cbf55fd6384a___RS_L.Scorch 1112.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9a86e092-62b7-4072-be7b-5499b0cfde01___RS_L.Scorch 0065.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/58cccdc8-e541-4b94-94e5-0e79c1d1c9c5___RS_L.Scorch 0899.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f7d50599-f99b-4b36-ada8-712428030a2e___RS_L.Scorch 0945.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c38d684b-233c-4fc8-a5d4-150143e94ab1___RS_L.Scorch 1420.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/98c90c8a-5532-4882-adb7-955754d00c46___RS_L.Scorch 1352.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/301b0319-e28a-44ee-836c-92a5e8a5b478___RS_L.Scorch 1558.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b882a8dc-1956-448f-890a-fd188b7cc194___RS_L.Scorch 1010.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bdf0a747-3514-46fc-9d9f-5f23c8546209___RS_L.Scorch 1388.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/36972d78-29ec-43b7-aa31-c9e2a08e4c35___RS_L.Scorch 1012.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5f583d1a-3504-4bf0-a3df-7cbc73faac19___RS_L.Scorch 1423.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/752dec50-2f1c-4b6c-be5a-b7b8f119dbd7___RS_L.Scorch 1270.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/35240d39-9516-4279-bc07-1d9fb39edcca___RS_L.Scorch 0814.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/131e39bc-35b5-4bf9-b895-6d50db8204dd___RS_L.Scorch 1343.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f10b17bb-4841-4c98-9557-27d866eed36a___RS_L.Scorch 0109.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/59def4d2-f75f-4550-875a-adafdf5b4ab9___RS_L.Scorch 0976.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/35499a79-86db-4eff-9d4a-30d7edde74cb___RS_L.Scorch 0058.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6476133e-4c18-40c4-9bca-36355a7512d2___RS_L.Scorch 0809.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7011a44e-fd95-478e-a4b2-4a3f5f8f08e2___RS_L.Scorch 0146.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70d2220e-f8b2-444f-8c75-12e49979d4c7___RS_L.Scorch 9980.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3c310ddf-f6fa-4e5e-98a8-d1273b75e518___RS_L.Scorch 0067.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/aa7594e0-4387-4c28-8f78-8e0dc9dc4e6f___RS_L.Scorch 1194.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d1143390-e5d3-44e7-a1c6-dfeb40da8eb0___RS_L.Scorch 1254.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/94f6ed8a-9bd1-4d29-b862-d195209d29d2___RS_L.Scorch 0027.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f3e99f65-fec8-45af-a2c0-196ad7188d70___RS_L.Scorch 1272.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4f08c5e9-c17c-4668-876c-4fd8fa271984___RS_L.Scorch 0042.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2272d3ae-de6d-4fcd-af45-098cb2cd0fdf___RS_L.Scorch 1086.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6daf8783-ab4d-451e-8aaf-00e29b23e29a___RS_L.Scorch 0009.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/71084f02-55a0-4b60-9ae9-2ea16df6bb93___RS_L.Scorch 0147.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cadecca1-c833-4e3f-847d-0c17f7b8972e___RS_L.Scorch 0102.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8ff7a1e6-f68f-4f06-a84c-f83e9ba12af7___RS_L.Scorch 1607.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/894e45ce-3e1d-4064-bdd3-409966f1bfbd___RS_L.Scorch 1479.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a4a68bfa-fef9-4e8f-a3e7-aeab239e210d___RS_L.Scorch 0061.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cdcf2f2c-2c59-4dbe-8b4e-a49c8dfec83a___RS_L.Scorch 1217.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/febb79c8-e178-411c-9f92-a384c34cf81c___RS_L.Scorch 0903.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/208522fa-0476-4bbe-9b37-4363a04da561___RS_L.Scorch 1150.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1415347e-7a77-4f48-94e2-6f2839b0badf___RS_L.Scorch 0908.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e208828b-bb26-4eb6-a49d-7f0280f443d3___RS_L.Scorch 0955.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7ba6bc57-b13e-4715-ab80-955bcd6eeec5___RS_L.Scorch 0940.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4c881e29-a78c-4b7d-9c75-5b72e29397de___RS_L.Scorch 0973.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c80f3d44-57ae-46ce-8f0e-f3ef5afe6eeb___RS_L.Scorch 0915.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/43e3bd88-9fb5-4a9d-bec4-6d80f7898b8c___RS_L.Scorch 0068.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/eec60b8d-2211-4672-996b-8242e816d60f___RS_L.Scorch 1605.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c7712210-979f-4de2-9c4f-7067831286be___RS_L.Scorch 9991.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2a49f9f9-acee-4c82-ba21-f51239735d74___RS_L.Scorch 0917.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0212ca0a-a34b-45f1-8a5b-73cde45acee0___RS_L.Scorch 1566.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/59913539-b991-45d9-a50b-a9cb840daa81___RS_L.Scorch 0919.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/88530a2d-30da-4c93-8dd7-f0535b9c948a___RS_L.Scorch 1557.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bfd17da3-f1c8-4a77-84b2-bf88edaff1e1___RS_L.Scorch 1455.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5f6ead43-4a36-4bd6-8ee6-75f405a40e67___RS_L.Scorch 0807.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/66eb909e-8046-44cd-95eb-d78ba3f91ccf___RS_L.Scorch 1124.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b492f881-f7dd-4d77-ad1b-e46a9b0b9b2c___RS_L.Scorch 1399.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b085a0c8-9a1d-4114-ad47-98f791a18012___RS_L.Scorch 1266.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d306834f-2362-49bc-aadb-8da5364d88f1___RS_L.Scorch 1334.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2c20ad16-5d7c-48dd-9c50-f504caf993d2___RS_L.Scorch 1197.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/39966ab3-c455-4ca2-ac9b-b3c4317ec37f___RS_L.Scorch 1329.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e0aacc2f-f7f4-4870-a706-92251ddde7d2___RS_L.Scorch 1590.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f07c8095-34d9-4039-adcd-ad02c7d88941___RS_L.Scorch 1613.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b0f29d46-77f6-4789-935f-95ac45b98f88___RS_L.Scorch 0796.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cee3f806-a484-4153-9a35-cb133d9ff33b___RS_L.Scorch 1177.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/59c99b9e-9646-49b3-b534-c8cf072d6837___RS_L.Scorch 0083.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7ef68428-ae8f-4cf6-b72a-8ebfb3e838b4___RS_L.Scorch 1235.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/264b9cb2-f632-4d66-bf0b-8f0f445866e9___RS_L.Scorch 9979.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f92e47ba-e50b-4fc1-af75-5e770ef26582___RS_L.Scorch 1092.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c5aa66d1-22cf-48af-a622-a9fbbd0d2c00___RS_L.Scorch 1499.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1634e251-6b10-4531-9df2-9f231cb7e047___RS_L.Scorch 0918.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70e522da-fc11-4fd0-b82d-c177b593b299___RS_L.Scorch 0144.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ca33c3f3-52d8-4401-8a6a-962a9b13f11d___RS_L.Scorch 9912.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7e20f02a-24d1-4200-9a37-53a4d8c14d86___RS_L.Scorch 0019.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70e14292-8be7-4452-89c4-366960b85d32___RS_L.Scorch 1424.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1081bac8-4ff2-4025-8328-446284666d47___RS_L.Scorch 1171.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a10983cc-18de-4fee-81f4-679da6e969d2___RS_L.Scorch 1507.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bdcb7650-7671-43e7-870d-d039fe4ec68f___RS_L.Scorch 0166.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dec3cbbe-0312-4e87-b223-23014a4b2994___RS_L.Scorch 0021.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8c12a342-d879-400f-975f-22f5ef3f32d2___RS_L.Scorch 1017.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/53fb0029-fa9c-4d81-9a35-b032faa02939___RS_L.Scorch 1357.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c18ccf4d-dbd1-4387-931d-d26b619543f2___RS_L.Scorch 1338.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8d8c7caf-b30d-4842-93ae-e0472e41a3e2___RS_L.Scorch 0161.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1a52e531-7868-4a49-b135-c4162bf6c036___RS_L.Scorch 1246.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d40005ab-21ad-4e6e-b2be-a428b1189ef9___RS_L.Scorch 1132.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/43ef6ba3-ebde-4656-92dd-1906ee79ddce___RS_L.Scorch 1564.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7492a475-3332-425e-a45c-848a668a2002___RS_L.Scorch 1123.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d4873067-614e-491b-9d0c-e71ae067bf73___RS_L.Scorch 1300.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/96112daa-d4f5-4d7a-b968-b6858eb4459b___RS_L.Scorch 9932.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/05063afb-0737-43d3-ae23-9bb42743bf7f___RS_L.Scorch 1097.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bc27498b-e23c-4f4f-b2ac-5202de687ef5___RS_L.Scorch 9955.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/480af90e-7176-4da3-9689-3942a20be40f___RS_L.Scorch 1463.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c42f9f75-1ec9-4dda-b718-de6a9e6b7538___RS_L.Scorch 1043.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/78f0cfb0-e3c5-4359-9d97-0aaf9620f510___RS_L.Scorch 1504.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7cd20e89-51cc-46ef-a34e-33bac0fcfae3___RS_L.Scorch 0959.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e35b3fba-967b-45dd-b9cd-db6c066c772c___RS_L.Scorch 0005.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/134e7f88-53dd-4c61-a385-938dbc66c46a___RS_L.Scorch 1514.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6faa454c-0462-4813-a001-3f71cf8a0148___RS_L.Scorch 1523.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2828fa61-2c57-48b8-afbc-f28567db7524___RS_L.Scorch 1203.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/62c2385e-8679-418e-8907-d0db613787db___RS_L.Scorch 0081.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1c6da3dd-a49b-41aa-bf9c-0f711ea66c5d___RS_L.Scorch 1615.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0a08af15-adfe-447c-8ed4-17ed2702d810___RS_L.Scorch 0054.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3d5c1145-b344-4965-afba-5e30fea822b5___RS_L.Scorch 1009.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/38cf78da-0e3d-47ee-9acd-8055452a4d7e___RS_L.Scorch 0975.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b788f53f-9320-4ec1-b4f1-3e7c66e0d1c2___RS_L.Scorch 1378.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/50ef3cab-ca8a-4c59-854f-8b84e68f6da2___RS_L.Scorch 9909.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/752d3a4c-a828-43ed-8256-28586b1fc0b9___RS_L.Scorch 1200.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5392e86c-d767-44f7-8043-8f90f970082a___RS_L.Scorch 1435.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3d2e77df-cf60-4736-9640-0939c88b89a2___RS_L.Scorch 1418.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bb7a9290-dba6-4a30-9da8-1a271e84ed3f___RS_L.Scorch 1560.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d75b11d5-4be2-4543-a2cf-11d1aed9755b___RS_L.Scorch 0999.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f1171a6b-2f53-40f9-b1e7-e35560f07834___RS_L.Scorch 1147.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/df88c663-910f-4079-8189-07ca57dc665a___RS_L.Scorch 0167.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5f4ad8ae-eeb3-4130-b671-e81d2c159b56___RS_L.Scorch 1188.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5231f1a9-85f9-4485-8147-d2f3d42b18ec___RS_L.Scorch 9999.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/115dc5a4-7689-49b6-908a-cfb86136db60___RS_L.Scorch 0780.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e381d91a-3def-41ab-ae5f-f156ea4975ed___RS_L.Scorch 1023.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/815a964f-f313-4acd-8b42-91e21f0cacb9___RS_L.Scorch 1397.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/290609a6-2e60-46b3-986e-1af09ff21e57___RS_L.Scorch 0863.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/62b39b7f-e1f5-4015-970c-825e38d3aab8___RS_L.Scorch 0949.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ac9a37bc-0023-46fe-b3ae-64600e60d567___RS_L.Scorch 1477.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9d7518b0-4a8b-4a64-9775-d8339f66d0c6___RS_L.Scorch 0960.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3760f75a-8ba2-4841-bc39-cfefa4509541___RS_L.Scorch 1063.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/253d61ac-85fd-4749-b071-3327f6d9384f___RS_L.Scorch 0082.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/722ea125-c593-4079-bcbb-b7440cb89da9___RS_L.Scorch 1497.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4e3d5499-017b-4de5-8087-d7f94caec399___RS_L.Scorch 9914.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/15f02e8e-503c-4e14-8728-51dc3d4a2f10___RS_L.Scorch 1527.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/974dcb97-08a8-4b8a-aa40-f9a02c9e2ec2___RS_L.Scorch 9944.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6a29fa42-1de4-4f30-9b59-961793ae719f___RS_L.Scorch 1317.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b361ca03-5d66-483b-b8ee-562ddc5a403f___RS_L.Scorch 0928.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9ba278ed-571c-4d15-899b-4c8b02fe554b___RS_L.Scorch 1304.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/93e0b7bd-ec6a-425f-b365-b66f4af7cb52___RS_L.Scorch 0933.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/233640f5-3b34-4739-9c80-9052872bc7b6___RS_L.Scorch 1447.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dd1c9d73-1f96-43b3-be6a-34f2a161a52f___RS_L.Scorch 1008.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/db0567df-5005-4064-93c2-49d5485a9f86___RS_L.Scorch 1093.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/51dad556-b8be-4e77-8bbd-427deab4eeba___RS_L.Scorch 1538.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5532c994-169e-4702-8772-a8e7c1eb1ee0___RS_L.Scorch 9907.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/231ee42e-ca7b-4c1f-9272-b963d445a966___RS_L.Scorch 9908.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3309393c-ef14-44e9-9594-2a9861f1b1b7___RS_L.Scorch 1505.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3cfcfd90-0c54-433e-addc-f9e2d30ae27c___RS_L.Scorch 1211.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/242657d2-7e24-4016-b837-10e7f905f02e___RS_L.Scorch 9998.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f0606fe2-72f3-4dda-9f21-05152daea27d___RS_L.Scorch 0886.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4434fd5c-8f45-40c4-9dfc-2e50a78c8a7b___RS_L.Scorch 0889.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5a83b4ae-e45d-4fe0-9231-38a547efbc56___RS_L.Scorch 1481.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bfe9051f-57f9-418c-880d-6b6aee0130c0___RS_L.Scorch 1585.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/65d2f749-dea4-496c-905a-344352ef6ad3___RS_L.Scorch 1098.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b99a6855-5c31-4733-8411-1001690423af___RS_L.Scorch 1099.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2d9ca0c9-5189-419b-b716-450dd8edffe2___RS_L.Scorch 1349.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e95174ec-e6f3-46e3-a5a6-9434a2021a79___RS_L.Scorch 1068.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9995e689-24e0-42a9-bd5e-0dd723b73b2e___RS_L.Scorch 1509.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c16c072d-4a26-4184-8844-ec6eeac1bd5a___RS_L.Scorch 1319.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8c6dbc42-33b9-4b89-bc18-a6c54188942b___RS_L.Scorch 0914.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8af8d94f-a866-40a8-9846-1fec1f2b255b___RS_L.Scorch 1552.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8521b2b5-d5fa-42fe-ad72-d0f5c94b3ca8___RS_L.Scorch 9940.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0406315f-ef03-4f67-bee9-9583d2731760___RS_L.Scorch 0912.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5e4d4035-89da-4096-a74d-bc45770d80a4___RS_L.Scorch 0997.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a2cb6533-7fe2-44b9-bf05-566d0298625d___RS_L.Scorch 0142.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/11d22595-35dc-46be-9436-79bcacb8880a___RS_L.Scorch 9953.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8db1878c-df56-44dc-a7f1-123432ef58d2___RS_L.Scorch 1437.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b9ac8104-7814-4500-bfe2-bb0a195de50d___RS_L.Scorch 0896.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/951c42f8-6f8d-4191-9723-55a76eec222f___RS_L.Scorch 0812.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5251a1e3-9502-4f78-ad85-1983b62861b0___RS_L.Scorch 0987.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/533d08a9-3364-4d80-9b0f-85b653873237___RS_L.Scorch 0941.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1ae1d3c0-fba3-49a0-a7ff-0619e7596a6c___RS_L.Scorch 0986.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/28c8910e-96b7-469a-93e9-06e4b527bda1___RS_L.Scorch 0969.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d9f64df2-e244-4580-a643-f2cebc13c034___RS_L.Scorch 0874.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2b78f984-c7b8-45ad-a96b-d790de14b369___RS_L.Scorch 0797.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1672eb34-46ef-4987-a45b-7dd6e0d6a31d___RS_L.Scorch 0003.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/16dda8cd-ee04-426b-9bc7-b60d1520e614___RS_L.Scorch 0104.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/74dfe939-0365-47e6-ab39-9fb2b656cf39___RS_L.Scorch 1323.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/41ab9e68-0da1-4292-a58d-7423dd785b1f___RS_L.Scorch 1445.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/48dde4cb-db7e-4c20-85bc-dc48746bbffa___RS_L.Scorch 0145.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/71949ffd-d402-445b-8827-065b433bb5cb___RS_L.Scorch 1255.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c813028f-add4-4591-832c-72163c6ff701___RS_L.Scorch 9962.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9cf4b587-8733-4b7a-a036-f0559e66f10a___RS_L.Scorch 1467.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7e471a0b-a30c-4ab9-8f20-9363b558cebf___RS_L.Scorch 1416.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/22ec5a13-b81a-430c-aae5-86c46eb7f091___RS_L.Scorch 0862.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f912228b-7bc5-4914-a414-6f4c81aadaca___RS_L.Scorch 0172.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/23359934-8e33-43b5-882b-40c78f59d830___RS_L.Scorch 9918.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7435a709-e8cf-47df-a9a2-a6ec089d077f___RS_L.Scorch 1122.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1b616826-e312-46b9-995a-e84958084b80___RS_L.Scorch 0885.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/fac15e1c-0f8e-4467-8bdb-9494e8bb431e___RS_L.Scorch 9915.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/59d38c39-9a02-4b30-9896-e90abbd79e6f___RS_L.Scorch 1297.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e8671e60-f38d-4bce-9c34-90c2b7875397___RS_L.Scorch 1408.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b357a25b-7f80-45e4-93b5-1c68c593e0f4___RS_L.Scorch 1029.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8edcded5-b3d4-468c-a86d-a2dd81534a4c___RS_L.Scorch 1457.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3f21964d-583e-4773-9922-8f1cbc8c0e02___RS_L.Scorch 1377.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9051acbe-1e46-4a44-97d1-c921bce7f2ee___RS_L.Scorch 1101.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/de254b11-8a58-4ea9-82d3-19188c60ed1c___RS_L.Scorch 0111.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7014ff4d-61a0-427b-8ef0-390eb0ed0120___RS_L.Scorch 1069.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/41045bd3-e1b9-4f12-ae37-bc02ce949ea9___RS_L.Scorch 0173.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1ea9b86f-9f18-4364-9933-f33cb2312c11___RS_L.Scorch 1521.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d8653cab-a304-45c7-9a7a-c0b358a483a2___RS_L.Scorch 1108.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/08adf0a2-b86c-4c0c-b5b3-fcc7ac5447cb___RS_L.Scorch 1165.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/41c17ca1-4e7d-430f-ad74-4ca46ab1cba0___RS_L.Scorch 0137.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a1e269d3-9f90-4395-86fe-422c9c8e612e___RS_L.Scorch 9974.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cb7d8aec-9b86-4778-a62a-9544051b1eae___RS_L.Scorch 1021.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/662a99d6-deb1-45ad-a5a5-29227f4e9e1d___RS_L.Scorch 1193.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e0211aae-10d0-489b-8bd4-668645ee4b5d___RS_L.Scorch 0996.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c1f18830-3536-4082-87ce-1296af942a28___RS_L.Scorch 9920.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/73b8b390-0ae1-4510-b30b-e25248660f2f___RS_L.Scorch 0979.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0d02dec3-bad2-4dba-a2d5-480b6fdf3d7a___RS_L.Scorch 1157.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b4cd90ef-d3db-4680-9871-f7978b31d093___RS_L.Scorch 1052.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2f7698cc-c5e9-45e7-ad50-1f16688977ad___RS_L.Scorch 0074.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b82642a3-1995-41c2-95de-39cc5610dc80___RS_L.Scorch 1485.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5ac8e2d9-f9f7-49b1-ad79-f05a60d3ebd2___RS_L.Scorch 1173.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/30e43c7d-0621-4827-8a3a-3ae771996f1e___RS_L.Scorch 9959.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9aec2d41-309d-450a-93e8-d4c2e85b0b04___RS_L.Scorch 1441.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/85466662-ecf7-4010-8f10-e1fc5fc356fa___RS_L.Scorch 1526.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4803b86c-f74e-475e-a94b-57d7663b80a8___RS_L.Scorch 0866.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b91a707a-4ea5-4af7-a19e-bc7db68d6d55___RS_L.Scorch 0036.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70c7d7ed-2ca9-41f4-a19a-864e20f5bbb8___RS_L.Scorch 9977.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4e407e24-e738-407c-9b4e-a77e99f85a32___RS_L.Scorch 9975.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/87571940-c67c-4f7a-b0f8-34335ce31496___RS_L.Scorch 1456.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/84ee727c-d814-4f2c-8afc-5564ecb4e1a0___RS_L.Scorch 0953.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/432d8c13-039f-47e8-b02e-8ee4a283e6f4___RS_L.Scorch 0803.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5286ac37-b3ce-4177-bf6f-24ff545cabdd___RS_L.Scorch 1053.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f1c2c618-1be3-4fa3-b3ed-b88ca37f3ade___RS_L.Scorch 1516.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8385f8c3-730e-461f-94fc-3154a5b0e846___RS_L.Scorch 1264.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b3eb2228-f38e-4472-a39e-3e39917bac3d___RS_L.Scorch 9982.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9686d2bd-9399-48b7-a6e5-3ada318fa9ea___RS_L.Scorch 1126.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b607d594-ef1d-4cea-8638-48fcf971a748___RS_L.Scorch 9923.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3ffc87a3-7d3c-48ec-a001-60d9ef6f04f4___RS_L.Scorch 0128.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/33456e3e-53a3-4be9-be9c-64fe7cd821a8___RS_L.Scorch 1355.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5a58d8c7-e935-411d-9568-8cfb7a343177___RS_L.Scorch 0139.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/01cff44f-9564-42f7-9a29-3daa487b306a___RS_L.Scorch 1333.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2bdc3e10-2225-46b7-9808-ff6d9553901a___RS_L.Scorch 1543.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bcf52398-58a1-414e-afd7-ba7b0e26d66e___RS_L.Scorch 1110.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b19a0f33-3e7d-4347-a1fc-2467ce6bb8ae___RS_L.Scorch 1280.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5fc51dfe-82f0-477e-8280-dd1b00d7f134___RS_L.Scorch 1219.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c626dd6f-33c3-4b39-8c25-d8954db4ae11___RS_L.Scorch 0850.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3b3c1f78-4ac2-416e-a725-a5b2ace4fa8c___RS_L.Scorch 1520.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b0bdc303-ae9c-4f4f-842e-f44affb77993___RS_L.Scorch 0884.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6ce00687-0995-42a8-b93e-3273b800aa15___RS_L.Scorch 0854.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9000b097-ea05-452f-bbbd-b8a156c9a779___RS_L.Scorch 0041.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a97a6633-d0b0-4181-8ac1-c947f55644b2___RS_L.Scorch 1326.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ba6ac501-62e7-46ec-ad02-1b21e00ca850___RS_L.Scorch 0929.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c55c8d8d-3eb5-4347-ab96-650efffca023___RS_L.Scorch 1107.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/301e75f8-a15d-4b41-917a-934d04d6b0da___RS_L.Scorch 1593.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e4f0a3a5-2de6-4496-83fa-8a3c015eed00___RS_L.Scorch 1561.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/9e846d22-58de-43af-97b3-b52eec137369___RS_L.Scorch 1230.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/268b9962-a649-48ec-9162-6f9cf4bd6111___RS_L.Scorch 1454.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5793ad89-cf6c-4d56-9ea5-26162e1a5f6d___RS_L.Scorch 0138.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5fc91330-7005-4f57-9676-c57e653c8e12___RS_L.Scorch 1204.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c883953a-0380-4952-8eb4-35579bcff94b___RS_L.Scorch 0925.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/12709186-36c7-40d4-bdfb-516255291ada___RS_L.Scorch 9939.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0be84fb6-08e5-40a0-af47-b576e92c12f8___RS_L.Scorch 1001.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7c249f1b-15d5-4424-91cd-58b52e01cfdb___RS_L.Scorch 1141.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d5a6d4da-85ae-4098-aaf5-aa4d3fe5249a___RS_L.Scorch 1074.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ce8fc718-8129-4da7-8b13-c65f173d2f9d___RS_L.Scorch 1002.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/41315fa6-00ff-41ba-a2df-19e389fd42bc___RS_L.Scorch 1220.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/72098b1a-ef4a-47eb-bb1f-f23b126b7057___RS_L.Scorch 1324.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/ac7bb639-98a6-4c24-bbfe-af98bf814849___RS_L.Scorch 9972.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/688126b3-9647-472c-aaec-340a3ab41586___RS_L.Scorch 9899.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d9cf9a87-7249-4cba-b73a-44b89e73e5c1___RS_L.Scorch 1015.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0a04940f-3650-4e45-a33c-4e4652bf9ee1___RS_L.Scorch 1236.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8a935443-6300-4978-82c3-0b56743a5b25___RS_L.Scorch 0020.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/abcc6c2f-778b-40e0-b7d3-24698f02aa8a___RS_L.Scorch 0947.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/08a36568-798d-4e2e-a1a9-6341bee5d0e3___RS_L.Scorch 1494.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/fad5a494-fb9c-4255-befa-d53c629dbfa7___RS_L.Scorch 1487.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/b84c1035-be0e-493b-9af5-bb81000d5cb0___RS_L.Scorch 0898.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bbce8ec1-d353-4979-8ba6-aeafbad8d542___RS_L.Scorch 1295.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/359de5a6-ec09-4a2f-a50f-5b765140eb0b___RS_L.Scorch 1614.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/83178057-79d0-46ab-ad87-858829d76aa2___RS_L.Scorch 0006.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/360f4523-f772-46e9-a77b-d50f89b85fc0___RS_L.Scorch 1405.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f77fb02a-c6fe-42dc-a43c-fd7fa3340fb8___RS_L.Scorch 9936.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3fd551e9-7d34-454d-9352-2af3e5886d11___RS_L.Scorch 1062.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/6a19c4f6-e87a-4866-a468-3032fb20b699___RS_L.Scorch 1394.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8708d212-acf5-4771-82ee-73a34549f4e0___RS_L.Scorch 0011.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/625f5b12-cac4-446f-b6a9-109c1564fec7___RS_L.Scorch 0864.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0957e6d7-9bf3-40c6-9b51-a4b53b868dc8___RS_L.Scorch 1345.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/c632b029-5bf6-4c07-b57d-e32e78bf270b___RS_L.Scorch 1518.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0c6d5127-86d7-4956-a93e-09963cf52f7f___RS_L.Scorch 0913.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/608d19da-4e86-4c17-bd68-393ca0933d6b___RS_L.Scorch 0931.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5879315d-20ab-4f95-8a38-3459fd973d5a___RS_L.Scorch 1208.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/70912b91-8ed2-4ba8-a432-b9c3d3934a63___RS_L.Scorch 1309.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a45656fb-2251-4748-be24-527e4fb37c6f___RS_L.Scorch 0927.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1da8e38e-9d76-4f4d-b7de-2a60aeb462da___RS_L.Scorch 0839.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f7e38a09-77fc-4014-a4cd-7e496d45f5f1___RS_L.Scorch 1175.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f0afdb9e-9af4-4c0a-b1b0-67db12985d3d___RS_L.Scorch 1498.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/699a623f-3500-49e9-828a-4c56568aaa78___RS_L.Scorch 0141.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/41e49b9f-0df6-43aa-a504-a8fd8a133bf3___RS_L.Scorch 1313.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/5cae7329-07e1-4f26-818e-754a2c17b0f2___RS_L.Scorch 1496.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/dcefd563-296e-4dce-a4b1-b6e8cc488b0a___RS_L.Scorch 1120.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2f54dce1-5aa1-43fb-8eb4-8aeb18419440___RS_L.Scorch 0029.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0c8b8b44-486b-423f-9a9e-af915d9f85c2___RS_L.Scorch 1407.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/aae0ff02-b80a-43f1-84d0-ee3db0ad0c9b___RS_L.Scorch 0160.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/95640426-92f4-4549-97b0-aa7b7d5fad8c___RS_L.Scorch 1495.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/2a986b90-e7a3-4cc3-be5f-a11808071417___RS_L.Scorch 0942.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/670e9f1b-0863-4ae9-b95e-e68f6899a4f5___RS_L.Scorch 0121.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/469e16af-7eb7-4b8e-b596-b9817151f13d___RS_L.Scorch 1512.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/24273ca3-0d5b-405e-903b-1d5485ef326e___RS_L.Scorch 1451.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/1d760f47-9d62-4ac4-8dfc-b157f9fbe5cf___RS_L.Scorch 1354.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3721afdd-4c45-4451-bda5-27b7006ec668___RS_L.Scorch 0156.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a13d559d-f908-4b00-b3a3-a0ebd6a15e57___RS_L.Scorch 1578.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/a522c9a1-15de-415b-8d7e-13568ee4d9e3___RS_L.Scorch 0964.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/641cd93f-6140-4fa7-9396-4206e55113f9___RS_L.Scorch 0978.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d6de134e-978b-4e4e-8a67-4138560b1b71___RS_L.Scorch 1095.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d89256de-8d57-49d0-9fa3-3be4a3bb3c20___RS_L.Scorch 9941.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/3ce5d5e5-3f64-45d2-b7ca-1b6d7141e5d8___RS_L.Scorch 1586.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/aa6f0946-8281-41d0-b5c7-778fe456ff1f___RS_L.Scorch 1046.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/f096d5a6-5e59-46ed-a05a-e77bb0eeb176___RS_L.Scorch 1238.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/38722737-ce6e-4934-a906-81c7d503b1c9___RS_L.Scorch 1380.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/24ed4d2a-098e-4313-b131-cf5ca2b49ab2___RS_L.Scorch 1602.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/8c5f9d15-9940-4523-a214-26eff4674724___RS_L.Scorch 0828.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/d4962999-c9c2-4cbd-b8c6-30807455ae31___RS_L.Scorch 1234.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/18c449e2-790e-4a6d-b745-a0d09b4e8910___RS_L.Scorch 1446.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/e6dad2a5-e5cc-4ef7-bbc8-5cb0d0f0cb0c___RS_L.Scorch 1594.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/4a516057-a035-4bfe-b6a2-a6978efe0837___RS_L.Scorch 0865.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/cc73f050-1676-4844-8fee-8ec57219b3c5___RS_L.Scorch 1216.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/7c2952f9-1801-4c11-9269-720b99d54d5a___RS_L.Scorch 0842.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/862d14c9-f2fa-416c-a19d-d6d84f957cc6___RS_L.Scorch 1565.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/819b4b18-965d-4944-a857-588242863457___RS_L.Scorch 0872.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/bdfbcdc7-71b3-474f-b437-1c4da0a68966___RS_L.Scorch 1096.JPG',\n",
       " '../data/raw/Strawberry___Leaf_scorch/0112c6b2-72cd-402b-89cd-ca052983403e___RS_L.Scorch 1161.JPG',\n",
       " ...]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_file_paths = utils.get_image_paths(\"../data/raw\", \"Strawberry\")\n",
    "image_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Strawberry___Leaf_scorch', 'Strawberry___healthy'], dtype='<U24')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = utils.get_class_names(\"../data/raw\", \"Strawberry\")\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [utils.get_label(file_path, class_names) for file_path in image_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_labels, test_labels = train_test_split(image_file_paths, labels, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, eval_files, train_labels, eval_labels = train_test_split(train_files, train_labels, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1000, eval: 251, test: 313, total: 1564\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(train_files)}, eval: {len(eval_files)}, test: {len(test_files)}, total: {len(image_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = {\"name\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 1}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    **blah\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species': 'Corn',\n",
       " 'num_classes': 4,\n",
       " 'class_names': ['Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
       "  'Corn_(maize)___Common_rust_',\n",
       "  'Corn_(maize)___Northern_Leaf_Blight',\n",
       "  'Corn_(maize)___healthy'],\n",
       " 'created_date': '2020-01-26 04:07:55',\n",
       " 'file_counts': {'train': 1618, 'test': 506, 'eval': 405}}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io.read_metadata(metadata_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
