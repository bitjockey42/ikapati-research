%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{url}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%Gummi|065|=)
\usepackage{url}
\title{\textbf{Capstone Project}}
\author{Allyson Julian}
\date{January 27, 2020}

\makeatother

\begin{document}
\maketitle

\section{Definition}

\subsection{Project Overview}

Advances in agricultural technology over the past 30 years have made
it easier for farmers to manage their farms, particularly when the
farms are comprised of multiple fields greater than 1000 acres in
size. The use of GPS on aerial imagery, drones, etc. have been instrumental
in precision agriculture \cite{liakos_machine_2018}.

But one of the challenges of modern farming is the detection of plant
diseases. For large farms in particular, it is time-consuming for
a farmer to manually check each growing plant for disease. It can
potentially be more cost-effective to diagnose plant diseases with
automated tools \cite{fujita_basic_2016}.

For this project, I built the Python library \textbf{ikapati}, which implements a multi-class classifier that functions as a plant disease detector and then deployed the best-performing model to an embedded system built specifically for use in Artificial Intelligence applications, a Jetson Nano robot.

The library also provides utility tools to do many of the tasks required in machine learning research, including the image preprocessing necessary to prepare the data for classification, scripts to train classifier itself, and utility functions to help evaluate performance.

\subsection{Problem Statement}

My main objective for this project was to build a library that can be used for both plant disease classification and can act as a framework with which to do further research in this subject area. It can be used to complete an entire machine learning pipeline from training to deployment to an embedded system.

To build this library, I needed to:

\begin{enumerate}
	\item Retrieve the PlantVillage Dataset (\url{https://github.com/spMohanty/PlantVillage-Dataset}).
	\item Prepare the raw color images from the PlantVillage Dataset for consumption by the model.
	\item Train the model to classify the different diseases for a given species of plant.
	\item Save the model as a TensorFlow Lite object for use in an embedded system.
\end{enumerate}

\subsection{Metrics}

Accuracy and loss were the main metrics used in this project.

\section{Analysis}

\subsection{Data Exploration}

The main datasets used were obtained from the PlantVillage Dataset: \url{https://github.com/spMohanty/PlantVillage-Dataset}

The dataset consists of 52,803 colored images with dimensions 256x256. The images
are split up into several folders, labelled according to the plant species (e.g. Peach) and disease class (e.g. Bacterial spot).

\subsection{Exploratory Visualization}

TODO: HERE PUT SOME IMAGES

\subsection{Algorithms and Techniques}

A \textbf{Convolutional Neural Network (CNN)} was selected as the classification algorithm due to its demonstrated efficacy in plant disease detection in previous studies \cite{toda_how_2019} \cite{fujita_basic_2016}.

Two CNN architectures were used in this project:

\begin{enumerate}
	\item is based on \textbf{AlexNet} \cite{krizhevsky_imagenet_2017} and is comprised of Conv2D (which does convolution), MaxPooling2D (max pooling), and FCD (which are a combination of Dense and Dropout layers).
	\item is based on the \textbf{InceptionV3} based network built in the study by Toda et al \cite{toda_how_2019} that demonstrated an accuracy of 99.99\% in predicting plant disease classification on the PlantVillage Dataset.
\end{enumerate}

\subsubsection{AlexNet}

This describes the layer sequence (the number indicates how many of that same layer repeats until the next one):

\begin{itemize}
	\item Conv2D (3) - Convolution Layer
	\item MaxPooling2D (1) - Max Pooling Layer
	\item Conv2D (2) - Convolution Layer
	\item MaxPooling2D (1) - Max Pooling Layer
	\item FCD (3) - Fully Connected Layer w/ optional Dropout.
\end{itemize}

\subsubsection{InceptionV3}

\begin{itemize}
	\item Conv2D (3) - Convolution Layer with Batch Normalization
	\item MaxPooling2D (1) - Max Pooling Layer
	\item Conv2D (2) - Convolution Layer with Batch Normalization
	\item MaxPooling2D (1) - Max Pooling Layer
	\item Mixed (9) - Sets of Conv2D with batch normalization
	\item GlobalAveragePooling2D (1) - Global Pooling Layer
	\item Dense (1) - The Output Layer
\end{itemize}

\subsubsection{Hyperparameters}

These parameters can be tuned/specified at training time:

\begin{itemize}
	\item \textbf{epochs} - the number of epochs to run.
	\item \textbf{learning rate} - the learning rate (optional for dynamic tuning).
	\item \textbf{batch size} - the number of training examples in each batch.
	\item \textbf{activation} - the activation function to use, e.g. "relu".
	\item \textbf{dropout} - the dropout rate, e.g. 0.2. This was only supported by AlexNet.
	\item \textbf{architecture} - the architecture to use for the CNN, "alexnet" (for AlexNet) or "inceptionv3" (for InceptionV3).
\end{itemize}

\subsection{Benchmark}

The benchmark for the plant disease classifier are results obtained by previous studies by
Toda et al, Fuentes et al, on plant detection which all utilize a CNN as a classifier\cite{toda_how_2019} \cite{fuentes_robust_2017}.

\section{Methodology}

\subsection{Data Preprocessing}

Preprocessing the data was completed using these steps:

\begin{enumerate}
	\item Get a list of all image filenames.
	\item Shuffle list of image filenames. 
	\item Get labels from the image folder names. For example: images that were in the Apple\_\_\_Apple\_scab were assigned the label Apple\_\_\_Apple\_scab.
	\item Split list of image filenames into training files (60\%), validation (20\%), and testing files (20\%) to mirror the split in a previous study \cite{toda_how_2019} .
	\item Create a training example for each image file so it can be added to a TFRecord (TensorFlow dataset).
	\item Write metadata describing the file counts of training, validation, and test sets and list the class names.
	\item Create parser function to read each TFRecord batch by batch during training due to hardware limitations (limited VRAM, in this case).
	\item Normalize image pixel values by dividing by 255 (representing the range of values for an RGB image) and subtracting -0.5 to the values to make them fall in the -0.5 to 0.5 range.
\end{enumerate}

\subsection{Implementation}

The library was primarily written in Python 3 and utilizes TensorFlow 2.0 (tf) for model training.

The modules for the \textbf{ikapati} library can be broken down as such:

\begin{itemize}
	\item \textbf{models} - these contain the training script and code to build the networks.
	\item \textbf{data} - this has the utility functions used to preprocess the data, e.g. normalizing pixel values, reading datasets into memory.
	\item \textbf{visualization} - this contains utility functions to help visualize and evaluate model performance. Figures included in this report were generated using that module.
\end{itemize}

\subsubsection{Data Preparation}

The datasets were prepared using the \textbf{ikapati/data/make\_dataset.py} script (which utilizes datasets scikit-learn and TensorFlow 2.0) and then saved as TFRecords, a format used by TensorFlow.

\begin{enumerate}
	\item Get the filenames of all the images that match the specified plant species.
	\item Follow the steps outlined in the \textbf{Data Preprocessing} section.
	\item Put each subset of training examples (training, validation, and test) into separate TFRecords (train.tfrecord, eval.tfrecord, test.tfrecord).
\end{enumerate}

\subsubsection{Training Stage}

The training models were created using TensorFlow 2.0 with the keras functional API. As described in the section \textbf{Algorithms and Techniques}, CNN was chosen as the classifier, with neural network architectures based on AlexNet and InceptionV3 as described in a previous study \cite{toda_how_2019} . TensorFlow was configured to use a physical GPU, a GeForce GTX 1080 Ti, to do the training.

The code used to build the AlexNet architecture was based on:

\begin{itemize}
    \item \url{https://engmrk.com/alexnet-implementation-using-keras/}
	\item \url{https://github.com/tensorpack/benchmarks/blob/master/other-wrappers/keras.alexnet.py}
\end{itemize}

The code used to build the InceptionV3 architecture was based on:

\begin{itemize}
    \item \url{https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_v3.py}
\end{itemize}

The training steps for each training run were:

\begin{enumerate}
	\item Execute the ikapati/models/train\_model.py script with the architecture (alexnet or inceptionv3), learning rate, epochs, batch size, activation, and dropout rate (if architecture is set to alexnet) hyperparameters specified, the model and data directories set, and checkpoint saving enabled to kick off a training run. (See the \textbf{Hyperparameters} described in the section \textbf{Algorithms and Techniques}d for more details on the hyperparameters).
	\item A folder is created under the model dir specified at runtime, with the UUID of dataset used as the name, and a subfolder within that with the start time string as the name. This will be where the models are saved during this training run.
	\item The start time of the training is recorded.
	\item After each epoch, if the validation loss has improved from the previous epoch, the model is saved to file in h5 format. Otherwise, we don't save anything and proceed with the next epoch.
	\item When all epochs have concluded, a final model is saved to file and the end time is recorded.
	\item Write the start and end time of the training run, the learning rate, and other parameters specified at runtime to a CSV file that acts as a log of training runs.
	\item Repeat steps above, tweaking the hyperparameters as needed.
\end{enumerate}

\subsubsection{Visualization and Metrics Tools}

Utility functions to evaluate model performance were created using seaborn (a high-level API for matplotlib) and pandas.

\subsubsection{Deployment to an Embedded System}

Upon training completion, trained models as saved in both h5 and tflite (TensorFlow Lite) formats. Both formats can be loaded for later use with the TensorFlow library, but the tflite format is specifically used to run model inference on embedded devices such as the Jetson Nano.

For this project, I built a robot driven by a Jetson Nano, an embedded system built specifically for use in Artificial Intelligence by NVIDIA.

\subsection{Refinement}

\section{Results}

\subsection{Justification}

\section{Conclusion}

\subsection{Visualization}

\subsection{Reflection}

\subsection{Improvement}

\bibliographystyle{IEEEtran}
\bibliography{citations}

\end{document}

\subsection{Model Evaluation and Validation}