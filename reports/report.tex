%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{url}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%Gummi|065|=)
\usepackage{url}
\title{\textbf{Capstone Project}}
\author{Allyson Julian}
\date{January 27, 2020}

\makeatother

\begin{document}
\maketitle

\section{Definition}

\subsection{Project Overview}

Advances in agricultural technology over the past 30 years have made
it easier for farmers to manage their farms, particularly when the
farms are comprised of multiple fields greater than 1000 acres in
size. The use of GPS on aerial imagery, drones, etc. have been instrumental
in precision agriculture \cite{liakos_machine_2018}.

But one of the challenges of modern farming is the detection of plant
diseases. For large farms in particular, it is time-consuming for
a farmer to manually check each growing plant for disease. It can
potentially be more cost-effective to diagnose plant diseases with
automated tools \cite{fujita_basic_2016}.

For this project, I built the Python library \textbf{ikapati}, which implements a multi-class classifier that functions as a plant disease detector.

The library also provides utility tools to do many of the tasks required in machine learning research, including the image preprocessing necessary to prepare the data for classification, scripts to train classifier itself, and utility functions to help evaluate performance.

\subsection{Problem Statement}

My main objective for this project was to build a library that implements plant disease classification and can act as a framework with which to do further research in this subject area. It can be used to complete an entire machine learning pipeline from training to deployment.

To build this library, I needed to:

\begin{enumerate}
	\item Retrieve the PlantVillage Dataset (\url{https://github.com/spMohanty/PlantVillage-Dataset}).
	\item Prepare the raw color images from the PlantVillage Dataset for consumption by the model.
	\item Train the model to classify the different diseases for a given species of plant.
	\item Save the model as a TensorFlow Lite object for use in an embedded system.
\end{enumerate}

\subsection{Metrics}

Accuracy and loss were the main metrics used in this project.

\section{Analysis}

\subsection{Data Exploration}

The main datasets used were obtained from the PlantVillage Dataset: \url{https://github.com/spMohanty/PlantVillage-Dataset}

The dataset consists of 20,638 images each of which have the dimensions
256 by 256 pixels and are all colored, non-greyscale JPEGs. The images
are split up into several folders, labelled according to the plant
species (e.g. Pepper Bell) and whether they are healthy or not (e.g.
Pepper bell healthy, Pepper bell Bacterial spot).

For this project, the colored photos were used for analysis and training. Due to time constraints, the scope of the project was narrowed down to one species - Tomato.

\subsection{Exploratory Visualization}

HERE PUT SOME IMAGES

\subsection{Algorithms and Techniques}

A \textbf{Convolutional Neural Network (CNN)} was selected as the classification algorithm due to its demonstrated efficacy in plant disease detection in previous studies \cite{toda_how_2019} \cite{fujita_basic_2016}.

The CNN architecture is based on \textbf{AlexNet} \cite{krizhevsky_imagenet_2017} and is comprised of Conv2D (which does convolution), MaxPooling2D (max pooling), and FCD (which are a combination of Dense and Dropout layers).

This describes the layer sequence (the number indicates how many of that same layer repeats until the next one):

\begin{itemize}
	\item Conv2D (3) - Convolution Layer
	\item MaxPooling2D (1) - Max Pooling Layer
	\item Conv2D (2) - Convolution Layer
	\item MaxPooling2D (1) - Max Pooling Layer
	\item FCD (3) - Fully Connected Layer w/ Dropout 
\end{itemize}


These parameters can be tuned/specified at training time:


\begin{itemize}
	\item \textbf{epochs} - the number of epochs to run.
	\item \textbf{learning rate} - the learning rate (optional for dynamic tuning).
	\item \textbf{batch size} - the number of training examples in each batch.
	\item \textbf{activation} - the activation function to use, e.g. "relu".
\end{itemize}

\subsection{Benchmark}

The benchmark for the classifier were results obtained by previous studies by
Toda et al, Fuentes et al, on plant detection which all utilize a CNN as a classifier\cite{toda_how_2019} \cite{fuentes_robust_2017}.



\section{Methodology}

\subsection{Data Preprocessing}

Preprocessing the data was completed using these steps:

\begin{enumerate}
	\item Shuffle list of image filenames.
	\item Get labels from filenames.
	\item Split list of image filenames into training files (80\%), validation (10\%), and testing files (10\%).
	\item Create a training example for each image file so it can be added to a TFRecord (TensorFlow dataset).
	\item Create parser function to read each TFRecord batch by batch during training due to hardware limitations (limited RAM, in this case).
	\item Normalize image pixel values by dividing by 255 (representing the range of values for an RGB image).
\end{enumerate}

\subsection{Implementation}

The library was primarily written in Python 3 and primarily utilizes TensorFlow 2.0 (tf) for training.

The modules for \textbf{ikapati} can be broken down as such:

\begin{itemize}
	\item \textbf{models} - these contain the training scripts and tools.
	\item \textbf{data} - this has the utility functions used to preprocess the data.
	\item \textbf{visualization} - this contains utility functions to help visualize and evaluate model performance.
\end{itemize}

\subsubsection{Data Preparation}

The datasets were prepared using scikit-learn and TensorFlow 2.0, and then saved as TFRecords:

\begin{enumerate}
	\item Get the filenames of all the images that match the specified plant species, in this case, "Tomato".
	\item Follow the steps outlined in the \textbf{Data Preprocessing} section.
	\item Put each subset of training examples (training, validation, and test) into separate TFRecords (train.tfrecord, eval.tfrecord, test.tfrecord).
\end{enumerate}

\subsubsection{Training Stage}

The training models were created using TensorFlow 2.0 with the keras functional API. As described in the section \textbf{Algorithms and Techniques}, CNN was chosen as the classifier, with AlexNet being the basis for the neural network architecture.

\begin{enumerate}
	\item Create a keras Sequential model, with the layers in this order: 3 Conv2D, 1 MaxPooling2D, 2 Conv2D, 1 MaxPooling2D, 3 FCDs, 1 Output Layer.
\end{enumerate}

\subsubsection{Visualization and Metrics Tools}

Utility functions to evaluate model performance were created using seaborn (a high-level API for matplotlib) and pandas.

\subsubsection{Model Saving and Deployment}

Upon training completion, 

\subsection{Refinement}

\section{Results}

\subsection{Justification}

\section{Conclusion}

\subsection{Visualization}

\subsection{Reflection}

\subsection{Improvement}

\bibliographystyle{IEEEtran}
\bibliography{citations}

\end{document}

\subsection{Model Evaluation and Validation}